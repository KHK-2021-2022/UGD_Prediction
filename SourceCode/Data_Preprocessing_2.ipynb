{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZTgCb4ki8Fo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpdEI9bbpX19"
      },
      "outputs": [],
      "source": [
        "# Copy from google drive to google collab\n",
        "# Change path\n",
        "!cp -r \"/content/drive/MyDrive/Test/Processed/Electronics.zip\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjSmzkFjSmBh"
      },
      "outputs": [],
      "source": [
        "# Copy from google drive to google collab\n",
        "# Change path\n",
        "#!cp -r \"/content/drive/MyDrive/Test/Processed/Movies_and_TV.zip\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSOErLZMpXZJ"
      },
      "outputs": [],
      "source": [
        "# Copy from google drive to google collab\n",
        "# Change path\n",
        "#!cp -r \"/content/drive/MyDrive/Test/Processed/CDs_and_Vinyl.zip\" \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVL7r8E1pXoT"
      },
      "outputs": [],
      "source": [
        "# Copy from google drive to google collab\n",
        "# Change path\n",
        "#!cp -r \"/content/drive/MyDrive/Test/Processed/Books.zip\" \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip into google collab drive"
      ],
      "metadata": {
        "id": "6UkHEG_DIVpl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT0DEgbITIOK"
      },
      "outputs": [],
      "source": [
        "#!unzip Movies_and_TV.zip -d '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uw-qZD30pJrx"
      },
      "outputs": [],
      "source": [
        "#!unzip CDs_and_Vinyl.zip -d '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BReUUUUpKHm"
      },
      "outputs": [],
      "source": [
        "#!unzip Books.zip -d '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXkBOsLdpKhA",
        "outputId": "369cf917-7411-4916-8c35-58eb69ee41eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  Electronics.zip\n",
            "  inflating: /content/content/Electronics.csv  \n"
          ]
        }
      ],
      "source": [
        "!unzip Electronics.zip -d '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJY_Z_N7YAJ_"
      },
      "outputs": [],
      "source": [
        "!pip install -q contractions\n",
        "!pip install -q pyspellchecker\n",
        "!pip install -q lexical_diversity\n",
        "!pip install -q jaccard_index\n",
        "!pip install -q distance\n",
        "!pip install -q swifter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApEQDtE1HIhO"
      },
      "outputs": [],
      "source": [
        "# Category to process: CD, BOOK, ELEC, MOVIE\n",
        "DATA_CATEGORY = 'ELEC'\n",
        "SAVE_FEATURE_ENGINEERING = True\n",
        "DATA_SPLIT = False\n",
        "\n",
        "if DATA_CATEGORY == 'CD':\n",
        "\n",
        "    DATA_NAME = 'CDs_and_Vinyl'\n",
        "\n",
        "elif DATA_CATEGORY == 'BOOK':\n",
        "\n",
        "    DATA_NAME = 'Books'\n",
        "\n",
        "elif DATA_CATEGORY == 'ELEC':\n",
        "\n",
        "    DATA_NAME = 'Electronics'\n",
        "\n",
        "elif DATA_CATEGORY == 'MOVIE':\n",
        "\n",
        "    DATA_NAME = 'Movies_and_TV'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MxiLr_lHYqH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize \n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import unicodedata\n",
        "import contractions\n",
        "import numpy as np\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "# Need to install via pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "from lexical_diversity import lex_div as ld\n",
        "from jaccard_index.jaccard import jaccard_index\n",
        "import distance\n",
        "import string\n",
        "import html as ihtml\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "import multiprocessing\n",
        "import swifter\n",
        "\n",
        "import sys\n",
        "# initialization\n",
        "spell = SpellChecker()\n",
        "\n",
        "stopwords_list = stopwords.words('english')\n",
        "stopwords_list += list(string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdcAHhAuHeOY"
      },
      "outputs": [],
      "source": [
        "path = '/content/' + DATA_NAME +'.csv'\n",
        "\n",
        "df = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTSoWyeeIBbT"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt9mKwQ1XSR8"
      },
      "source": [
        "# Data preprocessing and cleaning\n",
        "\n",
        "Note: Vote > 10 has been execute in the JSON Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkgOiyTXXk9G"
      },
      "outputs": [],
      "source": [
        "def strip_html(text):\n",
        "    text = str(text)\n",
        "    text = BeautifulSoup(ihtml.unescape(text), \"lxml\").text\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"http+\", \"\", text)\n",
        "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "# Define function to expand contractions\n",
        "def expand_contractions(text):\n",
        "    return(contractions.fix(text))\n",
        "\n",
        "# special_characters removal\n",
        "def remove_special_characters(text):\n",
        "    pattern = r'[^a-zA-z0-9\\s]'\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    return text\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "\n",
        "def remove_punctuation_and_splchars(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_word = remove_special_characters(new_word)\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords_list:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "\n",
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation_and_splchars(words)\n",
        "    words = remove_stopwords(words)\n",
        "    return words\n",
        "\n",
        "def lemmatize(words):\n",
        "    lemmas = lemmatize_verbs(words)\n",
        "    return lemmas\n",
        "\n",
        "def clean_text(input):\n",
        "    sample = denoise_text(input)\n",
        "    sample = expand_contractions(sample)\n",
        "    sample = remove_special_characters(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "def normalize_and_lemmaize(input):\n",
        "    sample = denoise_text(input)\n",
        "    sample = expand_contractions(sample)\n",
        "    sample = remove_special_characters(sample)\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    words = normalize(words)\n",
        "    lemmas = lemmatize(words)\n",
        "\n",
        "    return ' '.join(lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AqT4X3wXt1V"
      },
      "outputs": [],
      "source": [
        "# Calculate number of tagging: noun, adjective, etc.\n",
        "\n",
        "def get_number_of_words(text):\n",
        "  return len(text)\n",
        "\n",
        "def get_number_of_sentences(text):\n",
        "  sentences= nltk.sent_tokenize(text)\n",
        "  return len(sentences)\n",
        "\n",
        "def get_number_of_character(text):\n",
        "  return len(text)\n",
        "\n",
        "def get_number_of_question(text):\n",
        "  return len(text.split('? '))\n",
        "\n",
        "def get_number_of_exclamation(text):\n",
        "  return len(text.split('! '))\n",
        "\n",
        "def get_pos_tagging(text):\n",
        "  return([pos_tag[1] for pos_tag in pos_tag(text)])\n",
        "\n",
        "def get_number_of_upper(text):\n",
        "  return len([char for char in text if char.isupper()==True]) \n",
        "\n",
        "def get_polarity(text):\n",
        "  return TextBlob(text).sentiment.polarity\n",
        "\n",
        "def get_subjectivity(text):\n",
        "  return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "def get_number_of_misspell(tokens):\n",
        "  return len(spell.unknown(tokens))\n",
        "\n",
        "def get_number_of_noun(pos_tag):\n",
        "  return sum(1 for pos in pos_tag if pos.startswith('NN'))\n",
        "\n",
        "def get_number_of_adjective(pos_tag):\n",
        "  return sum(1 for pos in pos_tag if pos.startswith('JJ'))\n",
        "\n",
        "def get_number_of_adverb(pos_tag):\n",
        "  return sum(1 for pos in pos_tag if pos.startswith('RB'))\n",
        "\n",
        "def get_number_of_verb(pos_tag):\n",
        "  return sum(1 for pos in pos_tag if pos.startswith('VB'))\n",
        "\n",
        "def get_lex_diversity(token):\n",
        "  return(ld.ttr(token)) \n",
        "\n",
        "def get_num_one_letter(token):\n",
        "  return sum(1 for i in token if len(i) == 1)\n",
        "\n",
        "def get_num_two_letter(token):\n",
        "  return sum(1 for i in token if len(i) == 2)\n",
        "\n",
        "def get_num_long_letter(token):\n",
        "  return sum(1 for i in token if len(i) >= 3)\n",
        "\n",
        "def get_num_unique_word(token):\n",
        "  unique = set(token)\n",
        "  return len(unique)\n",
        "\n",
        "def get_num_stopword(token):\n",
        "  num_stopwords = [w for w in token if w in stopwords_list]\n",
        "\n",
        "  return len(num_stopwords)\n",
        "\n",
        "def get_cosine_similarity(title, cleanReview):\n",
        "  if len(cleanReview) == 0:\n",
        "    return 0\n",
        "  return 1 - distance.jaccard(title,cleanReview)\n",
        "\n",
        "def get_deviation_extreme(rate, min, max):\n",
        "  abs_min = abs(rate-min)\n",
        "  abs_max = abs(rate-max)\n",
        "\n",
        "  if (abs_min >= abs_max):\n",
        "    return abs_min\n",
        "  elif (abs_min < abs_max):\n",
        "    return abs_max\n",
        "\n",
        "def get_helpfulness(positive,totalVote,helpful,unhelpful):\n",
        "\n",
        "  if(positive/totalVote > helpful):\n",
        "    return 1\n",
        "  elif(positive/totalVote < unhelpful):\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rm4pkRY7RwC"
      },
      "source": [
        "# Check Nan and set as zero: reviewer_std_rating, product_std_rating\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIRiL73-jkFf"
      },
      "outputs": [],
      "source": [
        "# Before dropping data with NaN\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH5w_Q7PsWhD"
      },
      "outputs": [],
      "source": [
        "print('Total NaN in DataFrame', df.isnull().sum().sum())\n",
        "\n",
        "df.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
        "\n",
        "# Drop unwated column (Unnamed: 0)\n",
        "df = df.drop(df.columns[0],axis=1)\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf6X4Er_w6zk"
      },
      "outputs": [],
      "source": [
        "def word_sampling(df, n_sample, n_word, random_state):\n",
        "\n",
        "  df_subset = df.sample(n=n_sample, random_state = random_state)\n",
        "\n",
        "  df_subset['reviewText']=df_subset['reviewText'].apply(str)\n",
        "\n",
        "  df_subset['ori_cleaned_review'] = \\\n",
        "  df_subset['reviewText'].swifter.progress_bar(False).apply(lambda x:  \\\n",
        "                                                            clean_text(x))\n",
        "\n",
        "  df_subset['ori_cleaned_tokens'] =  \\\n",
        "    df_subset['ori_cleaned_review'].swifter.progress_bar(False).apply(lambda x: \\\n",
        "                                                        nltk.word_tokenize(x))\n",
        "\n",
        "  df_subset['word'] = \\\n",
        "    df_subset['ori_cleaned_tokens'].swifter.progress_bar(False).apply(lambda x:\\\n",
        "                                                          get_number_of_words(x))\n",
        "  # review without word will be filtered.  \n",
        "  df_subset = df_subset[df_subset['word'] > 0]\n",
        "\n",
        "  df_to_remove = df_subset[df_subset['word'] > n_word]\n",
        "\n",
        "  df_subset = df_subset.drop(df_to_remove.index)\n",
        "\n",
        "  n_standing = 0\n",
        "\n",
        "  while len(df_to_remove.index) != 0:\n",
        "\n",
        "    df = df.drop(df_to_remove.index)\n",
        "\n",
        "    n_outstanding = n_sample - len(df_subset.index)\n",
        "\n",
        "    df_outstanding = df.sample(n_outstanding, random_state = random_state)\n",
        "\n",
        "    df_outstanding['ori_cleaned_review'] = \\\n",
        "      df_outstanding['reviewText'].swifter.progress_bar(False).apply(lambda x:  clean_text(x))\n",
        "\n",
        "    df_outstanding['ori_cleaned_tokens'] =  \\\n",
        "      df_outstanding['ori_cleaned_review'].swifter.progress_bar(False).apply(lambda \\\n",
        "                                                      x:  nltk.word_tokenize(x))\n",
        "\n",
        "    df_outstanding['word'] = \\\n",
        "      df_outstanding['ori_cleaned_tokens'].swifter.progress_bar(False).apply(lambda x: \\\n",
        "                                                          get_number_of_words(x))\n",
        "\n",
        "    df_to_remove = df_outstanding[df_outstanding['word'] > n_word]\n",
        "\n",
        "    df_outstanding = df_outstanding.drop(df_to_remove.index)\n",
        "\n",
        "    df_subset = pd.concat([df_subset, df_outstanding])\n",
        "\n",
        "  return df_subset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ5gimJWy7cW"
      },
      "source": [
        "Data is order based on reviewer id therefore shuffling to make sure all reviewer, review and product can be covered. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN3Rrb72U9Eg"
      },
      "outputs": [],
      "source": [
        "df = df.sample(frac=1,random_state = 42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu9NhcS1xGo1"
      },
      "source": [
        "# Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDt3vinktixn"
      },
      "outputs": [],
      "source": [
        "# #Process 75 % ratio first\n",
        "\n",
        "if DATA_SPLIT == True:\n",
        "  df_75 = word_sampling(df = df[df['helpful_ratio'] > 0.75], n_sample = 50000, \\\n",
        "                      n_word = 500, random_state = 42)\n",
        "\n",
        "  df_35 = word_sampling(df = df[df['helpful_ratio'] < 0.35], n_sample = 50000, \\\n",
        "                      n_word = 500, random_state = 42)\n",
        "\n",
        "  #save memory\n",
        "  del df\n",
        "\n",
        "  df = pd.concat([df_75,df_35])\n",
        "\n",
        "  #save memory\n",
        "  del df_75, df_35\n",
        "\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  df.info()\n",
        "\n",
        "  df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eytjcNph2EiP"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak1i3_ea2Hga"
      },
      "outputs": [],
      "source": [
        "if DATA_SPLIT == False:\n",
        "\n",
        "  df['reviewText']=df['reviewText'].apply(str)\n",
        "\n",
        "  df['ori_cleaned_review'] = \\\n",
        "      df['reviewText'].swifter.progress_bar(True).apply(lambda x:  \\\n",
        "                                                            clean_text(x))\n",
        "\n",
        "  df['ori_cleaned_tokens'] =  \\\n",
        "      df['ori_cleaned_review'].swifter.progress_bar(True).apply(lambda x: \\\n",
        "                                                        nltk.word_tokenize(x))\n",
        "\n",
        "  df['word'] = \\\n",
        "      df['ori_cleaned_tokens'].swifter.progress_bar(True).apply(lambda x:\\\n",
        "                                                          get_number_of_words(x))\n",
        "\n",
        "  #Take only comment with maximum length 500 words, and more than 0 word    \n",
        "  df = df[(df['word'] <= 500) & (df['word'] > 0)]\n",
        "\n",
        "  df['char']    = df['ori_cleaned_review'].swifter.apply(lambda x: get_number_of_character(x))\n",
        "\n",
        "  df = df.drop('ori_cleaned_review',axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "H_FxS7eY2KhQ"
      },
      "outputs": [],
      "source": [
        "# structual features\n",
        "# 3 minutes run for 40k data\n",
        "\n",
        "# Prepare token\n",
        "\n",
        "df['original_tokens'] = df['reviewText'].swifter.apply(lambda x:  nltk.word_tokenize(x))\n",
        "\n",
        "# Original token processing  ======================= start\n",
        "df['spell'] = df['original_tokens'].swifter.apply(lambda x: get_number_of_misspell(x))\n",
        "# save memory space\n",
        "df = df.drop('original_tokens',axis=1)\n",
        "# Original token processing  ======================= end\n",
        "\n",
        "# original cleaned token  ======================= start\n",
        "# clean review\n",
        "\n",
        "df['num_one_letter'] = df['ori_cleaned_tokens'].swifter.apply(lambda x:  get_num_one_letter(x))\n",
        "\n",
        "\n",
        "df['num_two_letter'] = df['ori_cleaned_tokens'].swifter.apply(lambda x:  get_num_two_letter(x))\n",
        "\n",
        "df['num_long_letter'] = df['ori_cleaned_tokens'].swifter.apply(lambda x:  get_num_long_letter(x))\n",
        "\n",
        "df['num_stop_word'] = df['ori_cleaned_tokens'].swifter.apply(lambda x: get_num_stopword(x))\n",
        "# save memory space\n",
        "df = df.drop('ori_cleaned_tokens',axis=1)\n",
        "# original cleaned token  ======================= end\n",
        "\n",
        "# Normal review text\n",
        "\n",
        "df['sent']    = df['reviewText'].swifter.apply(lambda x: get_number_of_sentences(x))\n",
        "\n",
        "df['question'] = df['reviewText'].swifter.apply(lambda x: get_number_of_question(x))\n",
        "\n",
        "df['exclam']  = df['reviewText'].swifter.apply(lambda x: get_number_of_exclamation(x))\n",
        "\n",
        "df['caps']    = df['reviewText'].swifter.apply(lambda x: get_number_of_upper(x))\n",
        "\n",
        "# calculation\n",
        "df['len']     = df['char'] / df['word']\n",
        "df['avg']     = df['word'] / df['sent']\n",
        "df['per']     = df['question'] / df['sent']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_ZDGdl1w2Och"
      },
      "outputs": [],
      "source": [
        "df['norm_lem_Text'] = df['reviewText'].swifter.apply(lambda text: normalize_and_lemmaize(text))\n",
        "\n",
        "# norm_lem_Text processing ======================= start\n",
        "# Sentiment feature engineering\n",
        "\n",
        "df['polarity'] = df['norm_lem_Text'].swifter.apply(lambda text: get_polarity(text))\n",
        "\n",
        "df['subjectivity'] = df['norm_lem_Text'].swifter.apply(lambda text: get_subjectivity(text))\n",
        "\n",
        "df['norm_lem_tokens'] = df['norm_lem_Text'].swifter.apply(lambda text: nltk.word_tokenize(text))\n",
        "# norm_lem_Text processing ======================= end \n",
        "\n",
        "# save memory space\n",
        "df = df.drop('norm_lem_Text',axis=1)\n",
        "\n",
        "# norm_lem_token processing =========================== start\n",
        "\n",
        "df['pos'] = df['norm_lem_tokens'].swifter.apply(lambda text: get_pos_tagging(text))\n",
        "\n",
        "df['unique_word'] = df['norm_lem_tokens'].swifter.apply(lambda x: get_num_unique_word(x))\n",
        "\n",
        "# norm_lem_token processing =========================== end\n",
        "\n",
        "# POS processing ====================================== start\n",
        "\n",
        "df['pos_len'] = df['pos'].swifter.apply(lambda x: len(x))\n",
        "# Lexicol structural feature engineering\n",
        "\n",
        "df['noun'] = df['pos'].swifter.apply(lambda text: get_number_of_noun(text))\n",
        "\n",
        "df['noun_ratio'] =  df['noun'] / df['pos_len']\n",
        "\n",
        "df['adjective'] = df['pos'].swifter.apply(lambda text: get_number_of_adjective(text))\n",
        "\n",
        "df['adverb'] = df['pos'].swifter.apply(lambda text: get_number_of_adverb(text))\n",
        "\n",
        "df['adverb_ratio'] = df['adverb'] / df['pos_len']\n",
        "\n",
        "df['verb'] = df['pos'].swifter.apply(lambda text: get_number_of_verb(text))\n",
        "\n",
        "df['verb_ratio'] = df['verb'] / df['pos_len']\n",
        "\n",
        "df['lex_diversity'] =  df['pos'].swifter.apply(lambda text: get_lex_diversity(text))\n",
        "\n",
        "# save memory space\n",
        "df = df.drop('pos',axis=1)\n",
        "\n",
        "# To resolve 0 division issue\n",
        "\n",
        "# POS processing ====================================== end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxQ4bLGE3P0-"
      },
      "source": [
        "# Title/Summary Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KOw2DDsB2QaH"
      },
      "outputs": [],
      "source": [
        "# normalization and lemmaize\n",
        "\n",
        "df['summary']=df['summary'].apply(str)\n",
        "\n",
        "df['cleanSummary'] = df['summary'].swifter.apply(lambda text: normalize_and_lemmaize(text))\n",
        "\n",
        "# tokenize the summary/title\n",
        "\n",
        "df['clean_summary_tokens'] = df['cleanSummary'].swifter.apply(lambda text: nltk.word_tokenize(text))\n",
        "\n",
        "# feature engineering for summary/title\n",
        "\n",
        "df['title_cosine_similarity'] = df.swifter.apply(lambda x : get_cosine_similarity( x['clean_summary_tokens'],x['norm_lem_tokens']),axis=1)\n",
        "\n",
        "# norm_lem_tokens- save ram space\n",
        "df = df.drop('norm_lem_tokens',axis=1)\n",
        "\n",
        "df['title_Polarity'] = df['summary'].swifter.apply(lambda text: get_polarity(text))\n",
        "\n",
        "df['title_subjectivity'] = df['summary'].swifter.apply(lambda text: get_subjectivity(text))\n",
        "\n",
        "df['title_word']    = df['summary'].swifter.apply(lambda x: get_number_of_words(x))\n",
        "\n",
        "# save memory space\n",
        "df = df.drop(['cleanSummary', 'clean_summary_tokens'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CA804o8Xrrfq"
      },
      "outputs": [],
      "source": [
        "# To resolve NaN issue due to 0 devision\n",
        "df.replace([np.inf, -np.inf, np.nan], 0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUAPES4Q3BU9"
      },
      "source": [
        "# Label: Helpful or Unhelpful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NR5pRBJ7Sf_x"
      },
      "outputs": [],
      "source": [
        "df = df[df['total'] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TxZoVLOM2WdJ"
      },
      "outputs": [],
      "source": [
        "df['helpfulness'] = df.swifter.apply(lambda x : get_helpfulness(x['positive'],\\\n",
        "                                  x['total'], helpful = 0.75, \\\n",
        "                                  unhelpful = 0.35),axis=1).astype(\"category\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQGjJ3vdxKhd"
      },
      "source": [
        "# Save Data for LIWC processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y5VHUX_OQ8OD"
      },
      "outputs": [],
      "source": [
        "df.iloc[:,0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1OpYYE881Izb"
      },
      "outputs": [],
      "source": [
        "# Save the result from JSON preprocessing result\n",
        "# as csv to google collab\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "path = '/content/' + DATA_NAME + '_Almasdi_Processed.csv'\n",
        "\n",
        "with open(path, 'w', encoding = 'utf-8-sig') as f:\n",
        "    df.to_csv(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XF42wez01W3j"
      },
      "outputs": [],
      "source": [
        "# zip and save in google drive\n",
        "# change path name\n",
        "#!zip -r '/content/drive/MyDrive/Test/Processed/Movies_and_TV_Almasdi_Processed.zip' '/content/Movies_and_TV_Almasdi_Processed.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "46PZScHgpuNG"
      },
      "outputs": [],
      "source": [
        "# zip and save in google drive\n",
        "# change path name\n",
        "!zip -r '/content/drive/MyDrive/Test/Processed/CDs_and_Vinyl_Almasdi_Processed.zip' '/content/CDs_and_Vinyl_Almasdi_Processed.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_JjRrxvxpure"
      },
      "outputs": [],
      "source": [
        "# zip and save in google drive\n",
        "# change path name\n",
        "#!zip -r '/content/drive/MyDrive/Test/Processed/Electronics_Almasdi_Processed.zip' '/content/Electronics_Almasdi_Processed.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0LwTyXtZpvMh"
      },
      "outputs": [],
      "source": [
        "# zip and save in google drive\n",
        "# change path name\n",
        "#!zip -r '/content/drive/MyDrive/Test/Processed/Books_Almasdi_Processed.zip' '/content/Books_Almasdi_Processed.csv'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Data_Preprocessing_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}