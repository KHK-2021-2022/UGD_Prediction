{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t42ArLQbgnfr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTiQatsrEmPS"
      },
      "outputs": [],
      "source": [
        "# # # # # # # Copy and unzip from google drive to google collab\n",
        "# # # # # # # Change path\n",
        "# MOVIE\n",
        "!cp -r \"/content/drive/MyDrive/Test/LIWC/Movies_and_TV_LIWC_Processed.zip\" \"/content/\"\n",
        "!unzip \"/content/Movies_and_TV_LIWC_Processed.zip\" -d '/content/'\n",
        "\n",
        "#CD\n",
        "!cp -r \"/content/drive/MyDrive/Test/LIWC/CDs_and_Vinyl_LIWC_Processed.zip\" \"/content/\"\n",
        "!unzip \"/content/CDs_and_Vinyl_LIWC_Processed.zip\" -d '/content/'\n",
        "\n",
        "#BOOK\n",
        "!cp -r \"/content/drive/MyDrive/Test/LIWC/Books_LIWC_Processed.zip\" \"/content/\"\n",
        "!unzip \"/content/Books_LIWC_Processed.zip\" -d '/content/'\n",
        "\n",
        "# ELEC\n",
        "!cp -r \"/content/drive/MyDrive/Test/LIWC/Electronics_LIWC_Processed.zip\" \"/content/\"\n",
        "!unzip \"/content/Electronics_LIWC_Processed.zip\" -d '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tRegHPlFJ5z"
      },
      "outputs": [],
      "source": [
        " !pip -q install catboost\n",
        " !pip -q install git+https://github.com/eli5-org/eli5\n",
        " !pip -q install cmake\n",
        "\n",
        "# #lightGBM installation\n",
        " !git clone --recursive https://github.com/microsoft/LightGBM\n",
        " %cd /content/LightGBM\n",
        " !mkdir build\n",
        " !cmake -DUSE_GPU=1\n",
        " !make -j$(nproc)\n",
        " !sudo apt-get -y install python-pip\n",
        " !sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
        " %cd /content/LightGBM/python-package\n",
        " !sudo python setup.py install --precompile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L1EJWQuFEij"
      },
      "outputs": [],
      "source": [
        "# Category to process: CD, BOOK, ELEC, MOVIE\n",
        "DATA_CATEGORY = 'CD'\n",
        "HYPERPARAMETER = True # Run Hyperparameter\n",
        "NORMAL_PREDICTION = True # Run normal prediction\n",
        "FEATURE_IMPORTANT = True # Run and Get Feature Importance (CatBoost is the best)\n",
        "ALL_CATEGORIES = True # Run for all data categories\n",
        "ALL_DATA = True # Use all data split\n",
        "DATA_DISTRIBUTION = True # Show data distribution for data set\n",
        "FEATURE_MAJORITY_PREDICTION = True\n",
        "CV_FEATURE_MAJORITY_PREDICTION = True\n",
        "SHOW_PRED_RESULT = True\n",
        "SEED = 42\n",
        "EDA = True\n",
        "CROSSVALIDATION = True\n",
        "\n",
        "if DATA_CATEGORY == 'CD':\n",
        "\n",
        "    DATA_NAME = 'CDs_and_Vinyl'\n",
        "\n",
        "elif DATA_CATEGORY == 'BOOK':\n",
        "\n",
        "    DATA_NAME = 'Books'\n",
        "\n",
        "elif DATA_CATEGORY == 'ELEC':\n",
        "\n",
        "    DATA_NAME = 'Electronics'\n",
        "\n",
        "elif DATA_CATEGORY == 'MOVIE':\n",
        "\n",
        "    DATA_NAME = 'Movies_and_TV'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpVmMHp_FPv_"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve, learning_curve\n",
        "from sklearn import model_selection, preprocessing, metrics\n",
        "from sklearn.datasets import make_friedman1\n",
        "from sklearn.feature_selection import RFE, RFECV\n",
        "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "import lightgbm as lgb\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import uniform as sp_uniform\n",
        "\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import random\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.datasets import make_friedman1\n",
        "from sklearn.feature_selection import RFE, RFECV\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oYamPSjGHyd"
      },
      "source": [
        "### Data Preprocessing After LIWC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66BSQmoAGLxT"
      },
      "outputs": [],
      "source": [
        "if ALL_CATEGORIES == False:\n",
        "  path = '/content/' + DATA_NAME + '_LIWC_Processed.csv'\n",
        "\n",
        "  df = pd.read_csv(path)\n",
        "\n",
        "  import re\n",
        "  df = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "  print(df.shape)\n",
        "\n",
        "else: \n",
        "\n",
        "    path = '/content/CDs_and_Vinyl_LIWC_Processed.csv'\n",
        "\n",
        "    cd_df = pd.read_csv(path)\n",
        "\n",
        "    cd_df = cd_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "    print('CDs data shape: ' + str(cd_df.shape))\n",
        "\n",
        "    path = '/content/Books_LIWC_Processed.csv'\n",
        "\n",
        "    book_df = pd.read_csv(path)\n",
        "\n",
        "    book_df = book_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "    print('Books data shape: ' + str(book_df.shape))\n",
        "\n",
        "    path = '/content/Electronics_LIWC_Processed.csv'\n",
        "\n",
        "    elec_df = pd.read_csv(path)\n",
        "\n",
        "    elec_df = elec_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "    print('Electronics data shape: ' + str(elec_df.shape))\n",
        "\n",
        "    path = '/content/Movies_and_TV_LIWC_Processed.csv'\n",
        "\n",
        "    movie_df = pd.read_csv(path)\n",
        "\n",
        "    movie_df = movie_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "\n",
        "    print('Movie data shape: ' + str(movie_df.shape))\n",
        "\n",
        "    #baseline dataset\n",
        "\n",
        "    df =  cd_df.copy()\n",
        "\n",
        "    print('Baseline data shape: ' + str(df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGuyh0KvGTcY"
      },
      "outputs": [],
      "source": [
        "# Make a copy of data from csv file\n",
        "pred_df = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02vRV2FUVEpa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import numpy as np\n",
        "\n",
        "def show_distribution(data, data_name):\n",
        "  # Generate random data\n",
        "  #data = df['total']\n",
        "\n",
        "  # Colours for different percentiles\n",
        "  perc_25_colour = 'gold'\n",
        "  perc_50_colour = 'mediumaquamarine'\n",
        "  perc_75_colour = 'deepskyblue'\n",
        "  perc_95_colour = 'peachpuff'\n",
        "\n",
        "  # Plot the Histogram from the random data\n",
        "  fig, ax = plt.subplots(figsize=(25,13))\n",
        "\n",
        "  '''\n",
        "  counts  = numpy.ndarray of count of data ponts for each bin/column in the histogram\n",
        "  bins    = numpy.ndarray of bin edge/range values\n",
        "  patches = a list of Patch objects.\n",
        "        each Patch object contains a Rectnagle object. \n",
        "        e.g. Rectangle(xy=(-2.51953, 0), width=0.501013, height=3, angle=0)\n",
        "  '''\n",
        "  counts, bins, patches = ax.hist(data, facecolor=perc_50_colour, edgecolor='gray')\n",
        "\n",
        "\n",
        "  # Set the ticks to be at the edges of the bins.\n",
        "  ax.set_xticks(bins.round(2))\n",
        "  plt.xticks(rotation=70)\n",
        "\n",
        "  # Set the graph title and axes titles\n",
        "  plt.title('{}: Distribution of Helpful Ratio'.format(data_name), fontsize=25)\n",
        "  plt.ylabel('Count', fontsize=15)\n",
        "  plt.xlabel('Helpful Ratio', fontsize=2)\n",
        "        \n",
        "  # Calculate bar centre to display the count of data points and %\n",
        "  bin_x_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
        "  bin_y_centers = ax.get_yticks()[1] * 0.25\n",
        "\n",
        "  # Display the the count of data points and % for each bar in histogram\n",
        "  for i in range(len(bins)-1):\n",
        "    bin_label = \"{0:,.0f}\".format(counts[i])\n",
        "    plt.text(bin_x_centers[i], bin_y_centers, bin_label, rotation=90, rotation_mode='anchor')\n",
        "\n",
        "  # Display the graph\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgRtHWMNVLuY"
      },
      "outputs": [],
      "source": [
        "if DATA_DISTRIBUTION == True:\n",
        "    j = 0\n",
        "\n",
        "    for data_set in [cd_df, book_df, elec_df, movie_df]:\n",
        "\n",
        "      j += 1\n",
        "\n",
        "      if j == 1:\n",
        "        data_name = 'CDs'\n",
        "      elif j == 2:\n",
        "        data_name = 'Books'\n",
        "      elif j == 3:\n",
        "        data_name = 'Electronics'\n",
        "      elif j == 4:\n",
        "        data_name = 'Movies and TVs'\n",
        "\n",
        "      show_distribution(data_set['helpful_ratio'], data_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4xpl14W4jf3"
      },
      "outputs": [],
      "source": [
        "def show_values(axs, orient=\"v\", space=.01, decimal=4):\n",
        "    def _single(ax):\n",
        "        if orient == \"v\":\n",
        "            for p in ax.patches:\n",
        "                _x = p.get_x() + p.get_width() / 2\n",
        "                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n",
        "                if decimal != 0: \n",
        "                  value = '{:.4f}'.format(p.get_height())\n",
        "                else:\n",
        "                  value = '{:.0f}'.format(p.get_height())\n",
        "                ax.text(_x, _y, value, ha=\"center\") \n",
        "        elif orient == \"h\":\n",
        "            for p in ax.patches:\n",
        "                _x = p.get_x() + p.get_width() + float(space)\n",
        "                _y = p.get_y() + p.get_height() - (p.get_height()*0.5)\n",
        "                value = '{:.4f}'.format(p.get_width())\n",
        "                ax.text(_x, _y, value, ha=\"left\")\n",
        "\n",
        "    if isinstance(axs, np.ndarray):\n",
        "        for idx, ax in np.ndenumerate(axs):\n",
        "            _single(ax)\n",
        "    else:\n",
        "        _single(axs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QpE160vNY_q"
      },
      "outputs": [],
      "source": [
        "if DATA_DISTRIBUTION == True:\n",
        "      \n",
        "    j = 0\n",
        "\n",
        "    helpful = ['Unhelpful Review', 'Helpful Review']\n",
        "\n",
        "    for data_set in [cd_df, book_df, elec_df, movie_df]:\n",
        "\n",
        "        j += 1\n",
        "\n",
        "        if j == 1:\n",
        "          data_name = 'CDs'\n",
        "        elif j == 2:\n",
        "          data_name = 'Books'\n",
        "        elif j == 3:\n",
        "          data_name = 'Electronics'\n",
        "        elif j == 4:\n",
        "          data_name = 'Movies and TVs'\n",
        "        \n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(35, 13))\n",
        "\n",
        "        sns.set(font_scale=2)\n",
        "\n",
        "        p = sns.countplot(data=data_set, x='helpfulness')\n",
        "\n",
        "        show_values(p, decimal=0)\n",
        "\n",
        "        ax.set(xticks=range(len(helpful)), xticklabels=[i for i in helpful])\n",
        "\n",
        "        plt.title('{}: Helpfulness Count Distribution'.format(data_name))\n",
        "        plt.xlabel('Helpful')\n",
        "        plt.ylabel('Count')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbgcn2WUSiSc"
      },
      "outputs": [],
      "source": [
        "if DATA_DISTRIBUTION == True:\n",
        "      \n",
        "    j = 0\n",
        "\n",
        "    for data_set in [cd_df, book_df, elec_df, movie_df]:\n",
        "\n",
        "        j += 1\n",
        "\n",
        "        if j == 1:\n",
        "          data_name = 'CDs'\n",
        "        elif j == 2:\n",
        "          data_name = 'Books'\n",
        "        elif j == 3:\n",
        "          data_name = 'Electronics'\n",
        "        elif j == 4:\n",
        "          data_name = 'Movies and TVs'\n",
        "        \n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(35, 13))\n",
        "\n",
        "        sns.set(font_scale=2)\n",
        "\n",
        "        p = sns.countplot(data=data_set, x='helpful_ratio')\n",
        "\n",
        "        plt.title('{}: Helpfulness Count Distribution'.format(data_name))\n",
        "\n",
        "        plt.xlabel('Data')\n",
        "        plt.ylabel('Count')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT2Aqo5EGQLJ"
      },
      "outputs": [],
      "source": [
        "#Remove unwanted fields\n",
        "cols = [\n",
        "'Unnamed0',    # Unrelated fields\n",
        "'helpful_ratio', # helpful related feature will get close to 1\n",
        "'positive',      # helpful related feature will get close to 1\n",
        "'total',         # helpful related feature will get close to 1\n",
        "'Segment'        # generated by LIWC for parallel processing\n",
        "]\n",
        "\n",
        "pred_df = pred_df.drop(cols,axis=1).copy()\n",
        "\n",
        "if ALL_CATEGORIES == True:\n",
        "  cd_df = cd_df.drop(cols,axis=1).copy()\n",
        "  book_df = book_df.drop(cols,axis=1).copy()\n",
        "  elec_df = elec_df.drop(cols,axis=1).copy()\n",
        "  movie_df = movie_df.drop(cols,axis=1).copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p93f85CFJ6sU"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xodrXQeFGb7_"
      },
      "source": [
        "### Data Frame after deleting unwanted column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hm0GWlcGd0v"
      },
      "outputs": [],
      "source": [
        "pred_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aXTEtJ0Gr4U"
      },
      "outputs": [],
      "source": [
        "if DATA_DISTRIBUTION == True:\n",
        "  for i in range(0,140,20):\n",
        "    if i == 0:\n",
        "      print(pred_df.iloc[:,i:i+20].describe())\n",
        "    else:\n",
        "      print(pred_df.iloc[:,i+1:i+20].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpUQoQQcG4Bj"
      },
      "outputs": [],
      "source": [
        "def show_data_distribution(df_distribute, show=False):\n",
        "  # import matplotlib.pyplot as plt\n",
        "\n",
        "  # for column in pred_df:\n",
        "  #   with plt.rc_context(rc={'figure.max_open_warning': 0}):\n",
        "  #     plt.figure()\n",
        "  #     pred_df.boxplot([column])\n",
        "  if show == True:\n",
        "    \n",
        "    sns.set_style('dark')\n",
        "    fig = plt.figure(figsize= (20,40))\n",
        "    fig.subplots_adjust(hspace = 0.30, wspace = 0.30)\n",
        "    k=0\n",
        "    for i in range(1,34):\n",
        "        ax = fig.add_subplot(11,3,i)\n",
        "        sns.boxplot(x = df_distribute.iloc[:,k])\n",
        "        k = k + 1\n",
        "        if k == len(df_distribute.columns): break\n",
        "    plt.show()\n",
        "\n",
        "    sns.set_style('dark')\n",
        "    fig = plt.figure(figsize= (20,40))\n",
        "    fig.subplots_adjust(hspace = 0.30, wspace = 0.30)\n",
        "    k=34\n",
        "    for i in range(1,34):\n",
        "        ax = fig.add_subplot(11,3,i)\n",
        "        sns.boxplot(x = df_distribute.iloc[:,k])\n",
        "        k = k + 1\n",
        "        if k == len(df_distribute.columns): break\n",
        "    plt.show()\n",
        "\n",
        "    sns.set_style('dark')\n",
        "    fig = plt.figure(figsize= (20,40))\n",
        "    fig.subplots_adjust(hspace = 0.30, wspace = 0.30)\n",
        "    k=68\n",
        "    for i in range(1,34):\n",
        "        ax = fig.add_subplot(11,3,i)\n",
        "        sns.boxplot(x = df_distribute.iloc[:,k])\n",
        "        k = k + 1\n",
        "        if k == len(df_distribute.columns): break\n",
        "    plt.show()\n",
        "\n",
        "    sns.set_style('dark')\n",
        "    fig = plt.figure(figsize= (20,40))\n",
        "    fig.subplots_adjust(hspace = 0.30, wspace = 0.30)\n",
        "    k=102\n",
        "    for i in range(1,34):\n",
        "        ax = fig.add_subplot(11,3,i)\n",
        "        sns.boxplot(x = df_distribute.iloc[:,k])\n",
        "        k = k + 1\n",
        "        if k == len(df_distribute.columns): break\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK_uAyJLG4u3"
      },
      "outputs": [],
      "source": [
        "# Consider to loop to show data for all category\n",
        "show_data_distribution(pred_df, show = DATA_DISTRIBUTION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_uCyFFPHB5o"
      },
      "source": [
        "### Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzBMyJDGHEru"
      },
      "outputs": [],
      "source": [
        "def get_almasdi_data_split(pred_df):\n",
        "  # 90 k for training\n",
        "  # 45k for helpful\n",
        "  # 45k for unhelpful\n",
        "\n",
        "  # shuffle pred_df data\n",
        "\n",
        "  pred_df = pred_df.sample(frac=1, random_state=SEED)\n",
        "  pred_df = pred_df.reset_index(drop=True)\n",
        "\n",
        "# Train data =============== Start =============================================\n",
        "  train_helpful = pred_df[pred_df['helpfulness'] == 1].sample(n=45000, random_state = SEED)\n",
        "\n",
        "  pred_df.drop(train_helpful.index,inplace = True)\n",
        "\n",
        "  pred_df = pred_df.reset_index(drop=True)\n",
        "\n",
        "  train_unhelpful = pred_df[pred_df['helpfulness'] == 0].sample(n=45000, random_state = SEED)\n",
        "\n",
        "  pred_df.drop(train_unhelpful.index,inplace = True)\n",
        "\n",
        "  pred_df = pred_df.reset_index(drop=True)\n",
        "\n",
        "  train = pd.concat([train_helpful ,train_unhelpful])\n",
        "\n",
        "  train = train.reset_index(drop=True)\n",
        "\n",
        "# Train data ================= End ==========================================\n",
        "\n",
        "# Test data =============== Start =============================================\n",
        "\n",
        "  test_helpful = pred_df[pred_df['helpfulness'] == 1].sample(n=5000, random_state = SEED)\n",
        "\n",
        "  test_unhelpful = pred_df[pred_df['helpfulness'] == 0].sample(n=5000, random_state = SEED)  \n",
        "\n",
        "  test = pd.concat([test_helpful ,test_unhelpful])\n",
        "\n",
        "  test = test.reset_index(drop=True)\n",
        "\n",
        "# Test data ================= End =========================================\n",
        "\n",
        "# Hyperparameter data =============== Start ===================================\n",
        "  # shuffle the data\n",
        "  train = train.sample(frac=1, random_state=SEED)\n",
        "\n",
        "  X_hpo = train.sample(n=13500, random_state = SEED)\n",
        "\n",
        "  X_hpo = X_hpo.reset_index(drop=True)\n",
        "# Hyperparameter data =============== Start ===================================\n",
        "\n",
        "  # rebuild index\n",
        "\n",
        "  y_hpo = X_hpo['helpfulness']\n",
        "  X_hpo = X_hpo.drop('helpfulness', axis=1)\n",
        "\n",
        "  X_train = train.drop('helpfulness', axis=1)\n",
        "  y_train = train['helpfulness']\n",
        "\n",
        "  X_test = test.drop('helpfulness', axis=1)\n",
        "  y_test = test['helpfulness']\n",
        "\n",
        "  return X_train, y_train, X_test, y_test, X_hpo, y_hpo\n",
        "\n",
        "def get_standard_data_split(pred_df, oversampling = True):\n",
        "\n",
        "    # shuffle pred_df data\n",
        "    pred_df = pred_df.sample(frac=1, random_state=SEED)\n",
        "    pred_df = pred_df.reset_index(drop=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(pred_df.drop('helpfulness'\\\n",
        "                                  , axis=1), pred_df['helpfulness'], \n",
        "                                  test_size = 0.20, random_state=SEED)\n",
        "\n",
        "    X_hpo, X_hpo_test, y_hpo, y_hpo_test = train_test_split(X_train, y_train, \n",
        "                                  train_size = 0.15, random_state=SEED)\n",
        "    \n",
        "    if oversampling == True:\n",
        "        \n",
        "        sm = SMOTE(random_state=SEED)\n",
        "        X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n",
        "\n",
        "        X_train = X_train_res.copy()\n",
        "        y_train = y_train_res.copy()\n",
        "\n",
        "    return X_train, y_train, X_test, y_test, X_hpo, y_hpo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7kNbjL2Io03"
      },
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUaiibokHTYn"
      },
      "outputs": [],
      "source": [
        "# Class to extend the Sklearn classifier\n",
        "class SklearnHelper(object):\n",
        "    def __init__(self, clf, name=None, seed=SEED, params=None):\n",
        "        params['random_state'] = seed\n",
        "        self.clf = clf(**params)\n",
        "        self.name = name\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        self.clf.fit(x_train, y_train)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.clf.predict(x)\n",
        "    \n",
        "    def fit(self,x,y):\n",
        "        return self.clf.fit(x,y)\n",
        "    \n",
        "    def feature_importances(self):\n",
        "        return self.clf.feature_importances_\n",
        "\n",
        "    def cross_validation(self, X, y):\n",
        "        return cross_val_score(self.clf, X, y, cv=10,scoring='accuracy', verbose = 0)\n",
        "\n",
        "# # will show the accuracy for 0 till n feature. \n",
        "# # the feature is sorted according to their importance.\n",
        "    def get_fea_imp_exp_result(self, name, tech, fea_df, fea_result_df, min, max,step):\n",
        "\n",
        "         for i in range(min,max,step):\n",
        "\n",
        "            feature = fea_df.loc[0:(i-1),'Feature']   \n",
        "            print('Running: ' + str(name))\n",
        "            print('get_fea_imp_exp_result_loop : ' + str(i))\n",
        "\n",
        "            fea_imp_X_train = X_train[feature].copy()\n",
        "\n",
        "            self.fit(fea_imp_X_train, y_train)\n",
        "\n",
        "            cv_result = self.cross_validation(fea_imp_X_train, y_train)\n",
        "            print('Cross Validation Data Shape: ' + str(fea_imp_X_train.shape))\n",
        "            print(cv_result)\n",
        "\n",
        "            new_value = {'model': name,\n",
        "                        'technique': tech, \n",
        "                        'num_feature': feature.shape[0], \n",
        "                        'accuracy': np.mean(cv_result),\n",
        "                        'f1_score': '',\n",
        "                        'precision': '',\n",
        "                        'recall': ''}\n",
        "\n",
        "            fea_result_df = fea_result_df.append(new_value, ignore_index = True)\n",
        "\n",
        "         return fea_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqBawrLWJCFh"
      },
      "outputs": [],
      "source": [
        "books_xg_params = {'tree_method': 'gpu_hist', 'subsample': 1.0, 'max_depth': 10, 'learning_rate': 0.3, 'gamma': 0.4, 'colsample_bylevel': 0.5, 'verbose': -1} \n",
        "\n",
        "books_lg_params = {'top_rate': 0.2, 'other_rate': 0.1, 'num_leaves': 31, 'learning_rate': 0.3, 'feature_fraction_bynode': 0.75, 'verbose': -1} \n",
        "\n",
        "books_cat_params = {'leaf_estimation_iterations': 1, 'depth': 9, 'l2_leaf_reg': 9, 'learning_rate': 0.3, 'task_type': 'GPU', 'verbose': 0}\n",
        "\n",
        "cds_xg_params = {'tree_method': 'gpu_hist', 'subsample': 1.0, 'max_depth': 10, 'learning_rate': 0.3, 'gamma': 0.4, 'colsample_bylevel': 0.5, 'verbose': -1}\n",
        "\n",
        "cds_lg_params = {'top_rate': 0.2, 'other_rate': 0.05, 'num_leaves': 31, 'learning_rate': 0.2, 'feature_fraction_bynode': 0.75, 'verbose': -1} \n",
        "\n",
        "cds_cat_params = {'leaf_estimation_iterations': 10, 'depth': 6, 'l2_leaf_reg': 6, 'learning_rate': 0.1, 'task_type': 'GPU', 'verbose': 0} \n",
        "\n",
        "elec_xg_params = {'tree_method': 'gpu_hist', 'subsample': 1.0, 'max_depth': 10, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bylevel': 0.1, 'verbose': -1}\n",
        "\n",
        "elec_lg_params = {'top_rate': 0.2, 'other_rate': 0.05, 'num_leaves': 31, 'learning_rate': 0.2, 'feature_fraction_bynode': 0.75, 'verbose': -1}\n",
        "\n",
        "elec_cat_params = {'leaf_estimation_iterations': 10, 'depth': 9, 'l2_leaf_reg': 6, 'learning_rate': 0.2, 'task_type': 'GPU', 'verbose': 0}\n",
        "\n",
        "movie_xg_params = {'tree_method': 'gpu_hist', 'subsample': 1.0, 'max_depth': 10, 'learning_rate': 0.3, 'gamma': 0.4, 'colsample_bylevel': 0.5, 'verbose': -1}\n",
        "\n",
        "movie_lg_params =  {'top_rate': 0.6, 'other_rate': 0.3, 'num_leaves': 31, 'learning_rate': 0.2, 'feature_fraction_bynode': 1.0, 'verbose': -1} \n",
        "\n",
        "movie_cat_params = {'leaf_estimation_iterations': 10, 'depth': 9, 'l2_leaf_reg': 1, 'learning_rate': 0.1, 'task_type': 'GPU', 'verbose': 0}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34YEs670f4dk"
      },
      "source": [
        "XGBoost, LightGBM and Catboost feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgMmvtObIkdg"
      },
      "outputs": [],
      "source": [
        "def gen_model_fea_importance(xg, lg, cat, X_train, y_train, fea_result_df, random_state = SEED):\n",
        "\n",
        "\n",
        "  xg.fit(X_train,y_train)\n",
        "  lg.fit(X_train,y_train)\n",
        "  cat.fit(X_train,y_train)\n",
        "\n",
        "  # xg feature importance table =============== start =========\n",
        "  xg_fea_imp = pd.DataFrame({'value':xg.feature_importances(),\\\n",
        "                                    'Feature':X_train.columns})\n",
        "\n",
        "  xg_fea_imp = xg_fea_imp.sort_values(by=['value'], ascending=False)\n",
        "\n",
        "  xg_fea_imp.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # xg feature importance table =============== end =========\n",
        "\n",
        "  # lg feature importance table =============== start =========\n",
        "  lg_fea_imp = pd.DataFrame({'value':lg.feature_importances(),\\\n",
        "                                    'Feature':X_train.columns})\n",
        "\n",
        "  lg_fea_imp = lg_fea_imp.sort_values(by=['value'], ascending=False)\n",
        "\n",
        "  lg_fea_imp.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # lg feature importance table =============== end =========\n",
        "\n",
        "  # cat feature importance table =============== start =========\n",
        "  cat_fea_imp = pd.DataFrame({'value':cat.feature_importances(),\\\n",
        "                                    'Feature':X_train.columns})\n",
        "\n",
        "  cat_fea_imp = cat_fea_imp.sort_values(by=['value'], ascending=False)\n",
        "\n",
        "  cat_fea_imp.reset_index(drop=True, inplace=True)\n",
        "  # cat feature importance table =============== end =========\n",
        "\n",
        "  # Feature importance with XGBoost \n",
        "\n",
        "  print('Start Embedded - XGBoost')\n",
        "\n",
        "  fea_result_df = xg.get_fea_imp_exp_result(name='XGBoost', tech='XGBoost', \\\n",
        "                                                fea_df=xg_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = lg.get_fea_imp_exp_result(name='LightGBM', tech='XGBoost', \\\n",
        "                                                fea_df=xg_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = cat.get_fea_imp_exp_result(name='CatBoost', tech='XGBoost', \\\n",
        "                                                fea_df=xg_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "  # Feature importance with LightGBM\n",
        "\n",
        "  print('Start Embedded - LightGBM')\n",
        "\n",
        "  fea_result_df = xg.get_fea_imp_exp_result(name='XGBoost', tech='LightGBM', \\\n",
        "                                                fea_df=lg_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = lg.get_fea_imp_exp_result(name='LightGBM', tech='LightGBM', \\\n",
        "                                                fea_df=lg_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = cat.get_fea_imp_exp_result(name='CatBoost', tech='LightGBM', \\\n",
        "                                                fea_df=lg_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "  # Feature importance with CatBoost\n",
        "  print('Start Embedded - CatBoost')\n",
        "  fea_result_df = xg.get_fea_imp_exp_result(name='XGBoost', tech='CatBoost', \\\n",
        "                                                fea_df=cat_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = lg.get_fea_imp_exp_result(name='LightGBM', tech='CatBoost', \\\n",
        "                                                fea_df=cat_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = cat.get_fea_imp_exp_result(name='CatBoost', tech='CatBoost', \\\n",
        "                                                fea_df=cat_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df['num_feature'] = fea_result_df['num_feature'].astype(int)\n",
        "\n",
        "  return xg_fea_imp, lg_fea_imp, cat_fea_imp, fea_result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1aEUCicf_El"
      },
      "source": [
        "Mutual Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TlyUasQIsg8"
      },
      "outputs": [],
      "source": [
        "def gen_mutual_information(X_train, y_train, xg, lg, cat, fea_result_df, random_state = SEED):\n",
        "\n",
        "  print('Running MI')\n",
        "      \n",
        "  mutual_info = mutual_info_classif(X_train, y_train, random_state = SEED)\n",
        "\n",
        "  mutual_info = pd.Series(mutual_info)\n",
        "  mutual_info.index = X_train.columns\n",
        "\n",
        "  mi_fea_imp = pd.DataFrame({'value':mutual_info,\\\n",
        "                                    'Feature':mutual_info.index})\n",
        "\n",
        "  mi_fea_imp = mi_fea_imp.sort_values(by=['value'], ascending=False)\n",
        "\n",
        "  mi_fea_imp.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Feature importance 3 model\n",
        "  print('Running MI Prediction: XGBoost')\n",
        "  fea_result_df = xg.get_fea_imp_exp_result(name='XGBoost', tech='Mutual Information', \\\n",
        "                                                fea_df=mi_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  print('Running MI Prediction: LightGBM')\n",
        "  fea_result_df = lg.get_fea_imp_exp_result(name='LightGBM', tech='Mutual Information', \\\n",
        "                                                fea_df=mi_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  print('Running MI Prediction: catBoost')\n",
        "  fea_result_df = cat.get_fea_imp_exp_result(name='CatBoost', tech='Mutual Information', \\\n",
        "                                                fea_df=mi_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "  \n",
        "  return mi_fea_imp, fea_result_df, mutual_info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-HnBjUlZB2s"
      },
      "source": [
        "Permutation Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-03fuodZEzr"
      },
      "outputs": [],
      "source": [
        "def permutation_importance(model, X_val, y_val, seed=42, threshold=0.0005,\n",
        "                           minimize=True, verbose=True,):\n",
        "    results = {}\n",
        "\n",
        "    res_fea = pd.DataFrame()\n",
        "    y_pred = model.predict(X_val)\n",
        "    \n",
        "    results['base_score'] = accuracy_score(y_val,y_pred)\n",
        "    if verbose:\n",
        "        print(f'Base score {results[\"base_score\"]:.5}')\n",
        "\n",
        "    \n",
        "    for col in tqdm(X_val.columns):\n",
        "        freezed_col = X_val[col].copy()\n",
        "\n",
        "        X_val[col] = np.random.RandomState(seed = seed).permutation(X_val[col])\n",
        "        preds = model.predict(X_val)\n",
        "        results[col] = accuracy_score(y_val,preds)\n",
        "\n",
        "        X_val[col] = freezed_col\n",
        "        \n",
        "        if verbose:\n",
        "            print(f'column: {col} - {results[col]:.5}')\n",
        "\n",
        "        # Provide the permutation inforamtion ()\n",
        "        new_value = {\n",
        "            'value': results[col],\n",
        "            'Feature': col, \n",
        "        }\n",
        "\n",
        "        res_fea = res_fea.append(new_value, ignore_index = True)\n",
        "    \n",
        "    if minimize:\n",
        "        bad_features = [k for k in results if results[k] > results['base_score'] - threshold]\n",
        "    else:\n",
        "        bad_features = [k for k in results if results[k] < results['base_score'] + threshold]\n",
        "    bad_features.remove('base_score')\n",
        "    \n",
        "    return res_fea, results, bad_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0i2ZOZJZvc8"
      },
      "outputs": [],
      "source": [
        "def gen_permutation_importance(X_train, y_train, xg, lg, cat, fea_result_df, SEED):\n",
        "\n",
        "  xg.fit(X_train, y_train)\n",
        "\n",
        "  # the lowest the accuracy, the more important the field\n",
        "  res_fead, results, bad_features = permutation_importance(model=xg,\n",
        "                                                  X_val=X_train,\n",
        "                                                  y_val=y_train,\n",
        "                                                  verbose=False,\n",
        "                                                  seed = SEED)\n",
        "\n",
        "  pi_fea_imp = res_fead.sort_values(by=['value'], ascending=True)\n",
        "\n",
        "  pi_fea_imp.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # Feature importance with 3 model \n",
        "  fea_result_df = xg.get_fea_imp_exp_result(name='XGBoost', tech='Permutation Feature Importance', \\\n",
        "                                                fea_df=pi_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = lg.get_fea_imp_exp_result(name='LightGBM', tech='Permutation Feature Importance', \\\n",
        "                                                fea_df=pi_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = cat.get_fea_imp_exp_result(name='CatBoost', tech='Permutation Feature Importance', \\\n",
        "                                                fea_df=pi_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "  \n",
        "  return pi_fea_imp, fea_result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paaZiC86fo3L"
      },
      "source": [
        "### Combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cuSkfH3gZi_"
      },
      "outputs": [],
      "source": [
        "def get_column_to_delete(df, mutual_info):\n",
        "  mi_0 = mutual_info[df['level_0']]\n",
        "  mi_1 = mutual_info[df['level_1']]\n",
        "\n",
        "  if mi_0 < mi_1:\n",
        "    return [mutual_info[df['level_0']], mutual_info[df['level_1']], df['level_0']]\n",
        "  else:\n",
        "    return [mutual_info[df['level_0']], mutual_info[df['level_1']], df['level_1']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4dHz9d2frO_"
      },
      "outputs": [],
      "source": [
        "def gen_com_fea_importance(X_train, y_train, xg, lg, cat, fea_result_df, mutual_info, SEED):\n",
        "  # ================== find out correlation more than 0.7 ==== Start ================\n",
        "  df_corr = X_train.copy()\n",
        "\n",
        "  cor_matrix = df_corr.corr(method='spearman').abs()\n",
        "\n",
        "  #get only the top triangle \n",
        "  upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\n",
        "\n",
        "  to_drop = [column for column in upper_tri.columns if any(upper_tri[column] >= 0.7)]\n",
        "\n",
        "  corr_matrix = df_corr.corr().abs()\n",
        "\n",
        "  # Get field and their respective correlation\n",
        "  #the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\n",
        "\n",
        "  sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "                    .stack()\n",
        "                    .sort_values(ascending=False))\n",
        "\n",
        "  # Only equal or more than 0.7 will be considered for deletion\n",
        "  df_corr = pd.DataFrame(sol[sol.values > 0.7])\n",
        "\n",
        "  # get index as part of the field in data frame\n",
        "  df_corr.reset_index(inplace=True) \n",
        "\n",
        "  # ================== find out correlation more than 0.7 ==== End ================\n",
        "\n",
        "  # ======== Delete field pairing with more than 0.7 based on MI ==== Start =======\n",
        "\n",
        "  # Get Mutual information\n",
        "\n",
        "  # Drop the field with colleration > 0.7 based on mutual information\n",
        "  df_columns = df_corr.apply(lambda x: get_column_to_delete(x, mutual_info), axis=1)\n",
        "\n",
        "  df_corr[['level_0_mi', 'level_1_mi', 'column_to_delete']] = pd.DataFrame(df_columns.values.tolist())\n",
        "\n",
        "  column_to_delete = df_corr['column_to_delete']\n",
        "\n",
        "  column_to_delete = column_to_delete.unique()\n",
        "\n",
        "  print('Column to be remove  based on correlation and mutual information')\n",
        "  print(column_to_delete.size)\n",
        "\n",
        "  X_train_reduced = X_train.drop(column_to_delete,axis=1).copy()\n",
        "\n",
        "  print('Train data with feature reduced')\n",
        "  X_train_reduced.info()\n",
        "\n",
        "   # ======== Delete field pairing with more than 0.7 based on MI ==== end =======\n",
        "\n",
        "  # Find the permutation important again after deletion of 0.7 pairing based on MI\n",
        "\n",
        "  xg.fit(X_train_reduced, y_train)\n",
        "\n",
        "  # the lowest the accuracy, the more important the field\n",
        "  res_fead, results, bad_features = permutation_importance(model=xg,\n",
        "                                                  X_val=X_train_reduced,\n",
        "                                                  y_val=y_train,\n",
        "                                                  verbose=False,\n",
        "                                                  seed = SEED)\n",
        "\n",
        "  # sort based on permutation importance\n",
        "  # sorted Asecnding order, the lower the accuracy, the more important the feature \n",
        "  com_fea_imp = res_fead.sort_values(by=['value'], ascending=True)\n",
        "\n",
        "  com_fea_imp.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  fea_result_df = xg.get_fea_imp_exp_result(name='XGBoost', tech='Combination', \\\n",
        "                                                fea_df=com_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = lg.get_fea_imp_exp_result(name='LightGBM', tech='Combination', \\\n",
        "                                                fea_df=com_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "\n",
        "  fea_result_df = cat.get_fea_imp_exp_result(name='CatBoost', tech='Combination', \\\n",
        "                                                fea_df=com_fea_imp,\n",
        "                                                fea_result_df = fea_result_df,\n",
        "                                                min=9,max=161,step=10)\n",
        "  \n",
        "  return com_fea_imp, fea_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubmai9LcQf8B"
      },
      "outputs": [],
      "source": [
        "def gen_rfe_importance(X_train, y_train,fea_result_df, min, max, step, rfe_fea_imp_df, SEED = SEED):\n",
        "\n",
        "    xg = xgb.XGBClassifier (random_state=SEED, ctree_method= 'gpu_hist',\n",
        "                tree_method = 'gpu_hist', verbose = -1)\n",
        "    lg = lgb.LGBMClassifier (random_state=SEED,device = \"gpu\", verbose = -1, n_estimators = 100)\n",
        "    cat = CatBoostClassifier (random_seed=SEED,task_type = 'GPU', silent = True, \\\n",
        "                              eval_metric='Accuracy',early_stopping_rounds=3, iterations = 100)\n",
        "    \n",
        "    j = 0\n",
        "\n",
        "    for model in [xg, lg, cat]:\n",
        "      j += 1\n",
        "\n",
        "      if j == 1: \n",
        "          model_name = 'XGBoost'\n",
        "          print('Start RFE - XGBoost')\n",
        "      elif j == 2: \n",
        "          model_name = 'LightGBM'\n",
        "          print('Start RFE - LightGBM')\n",
        "      elif j == 3: \n",
        "          model_name = 'CatBoost'\n",
        "          print('Start RFE - CatBoost')\n",
        "\n",
        "      for i in range(min,max,step):\n",
        "\n",
        "          print('Number of i: ' + str(i))\n",
        "\n",
        "          selector = RFE(model, n_features_to_select=i, step=10)\n",
        "\n",
        "          selector = selector.fit(X_train, y_train)\n",
        "\n",
        "          features = X_train.columns[selector.support_]\n",
        "\n",
        "          new_feature = { 'model':model_name,\n",
        "              'no_feature' : selector.n_features_,\n",
        "              'name_of_selected_feature': features,\n",
        "              'rank_of_each_feature': selector.ranking_,\n",
        "              'support_of_each_feature': selector.support_\n",
        "          }\n",
        "\n",
        "          rfe_fea_imp_df = rfe_fea_imp_df.append(new_feature, ignore_index = True)\n",
        "\n",
        "          cv_accuracy = cross_val_score(model, X_train[features], y_train, cv=10,scoring='accuracy', verbose = 0)\n",
        "\n",
        "          new_value = {'model': model_name,\n",
        "                     'technique': 'RFE', \n",
        "                     'num_feature': i, \n",
        "                     'accuracy': np.mean(cv_accuracy) }\n",
        "\n",
        "          fea_result_df = fea_result_df.append(new_value, ignore_index = True)\n",
        "\n",
        "          print(fea_result_df)\n",
        "\n",
        "    return rfe_fea_imp_df, fea_result_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuIOyN77ILNx"
      },
      "source": [
        "### Result and Visualization for Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv_pmJQd2NFz"
      },
      "outputs": [],
      "source": [
        "if FEATURE_IMPORTANT == True:\n",
        "\n",
        "  xg_params = {'verbose' : -1,\n",
        "               'ctree_method' : 'gpu_hist',\n",
        "                'tree_method' : 'gpu_hist'\n",
        "                }\n",
        "\n",
        "  lg_params = {'verbose' : -1,\n",
        "               'device': 'gpu',\n",
        "               'n_estimators': 100\n",
        "               }\n",
        "\n",
        "  cat_params = {'verbose' : 0,\n",
        "                'task_type':'GPU',\n",
        "                'iterations': 100\n",
        "                }\n",
        "\n",
        "  xg = SklearnHelper(clf=xgb.XGBClassifier, seed=SEED, params=xg_params)\n",
        "  lg = SklearnHelper(clf=lgb.LGBMClassifier, seed=SEED, params=lg_params)\n",
        "  cat = SklearnHelper(clf=CatBoostClassifier, seed=SEED, params=cat_params)\n",
        "\n",
        "  j = 0\n",
        "\n",
        "  for i in [cd_df,book_df]:\n",
        "    #empty data frame to keep result of all feature importance prediction\n",
        "    fea_result_df = pd.DataFrame({\n",
        "     'model':[],\n",
        "      'technique': [],\n",
        "     'num_feature' : [],\n",
        "     'accuracy': [],\n",
        "     'f1_score': [],\n",
        "      'precision': [],\n",
        "      'recall': []\n",
        "     })\n",
        "    \n",
        "    rfe_fea_imp_df = pd.DataFrame({\n",
        "        'model':[],\n",
        "     'no_feature':[],\n",
        "      'name_of_selected_feature': [],\n",
        "      'rank_of_each_feature' : [],\n",
        "      'support_of_each_feature': []\n",
        "     })\n",
        "\n",
        "    j += 1\n",
        "\n",
        "    if j == 1:\n",
        "      data_cato_name = 'CDs'\n",
        "    elif j == 2:\n",
        "      data_cato_name = 'Books'\n",
        "    elif j == 3:\n",
        "      data_cato_name = 'Electronics'\n",
        "    elif j == 4:\n",
        "      data_cato_name = 'Movie'\n",
        "\n",
        "\n",
        "    if ALL_DATA == True:\n",
        "      X_train, y_train, X_test, y_test, X_hpo, y_hpo = get_standard_data_split(i.copy())\n",
        "    else:\n",
        "      X_train, y_train, X_test, y_test, X_hpo, y_hpo = get_almasdi_data_split(i.copy())\n",
        "\n",
        "    print('Checking pred_df: ' + str(fea_result_df.shape[0]))\n",
        "\n",
        "    print(data_cato_name + ' : ' + str(X_train.shape))\n",
        "\n",
        "    # #  RFE ============================== Start =======================\n",
        "\n",
        "    now = datetime.now()\n",
        "\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"RFE Current Time =\", current_time)\n",
        "\n",
        "    rfe_fea_imp_df, fea_result_df = gen_rfe_importance(X_train, y_train,fea_result_df, 9, 160, 10,rfe_fea_imp_df,SEED = SEED)\n",
        "\n",
        "    print('model fea result: ' + str(fea_result_df.shape[0]))\n",
        "    print('rfe fea important: ' + str(rfe_fea_imp_df.shape[0]))\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_rfe_fea_imp.csv'\n",
        "    rfe_fea_imp_df.to_csv(file_name)\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_fea_result_df.csv'\n",
        "    fea_result_df.to_csv(file_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"RFE Complete Time =\", current_time)\n",
        "\n",
        "    #  RFE ============================== End =======================\n",
        "\n",
        "    # # #  EMBEDDED ============================== Start =======================\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"EMBEDDED Current Time =\", current_time)\n",
        "\n",
        "    xg_fea_imp, lg_fea_imp, cat_fea_imp, fea_result_df = gen_model_fea_importance \\\n",
        "    (xg, lg, cat, X_train, y_train, fea_result_df,random_state = SEED)\n",
        "\n",
        "    print('model fea result: ' + str(fea_result_df.shape[0]))\n",
        "    print('xg fea important: ' + str(xg_fea_imp.shape[0]))\n",
        "    print('lg fea important: ' + str(lg_fea_imp.shape[0]))\n",
        "    print('cat fea important: ' + str(cat_fea_imp.shape[0]))\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_lg_fea_imp.csv'\n",
        "    lg_fea_imp.to_csv(file_name)\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_xg_fea_imp.csv'\n",
        "    xg_fea_imp.to_csv(file_name)\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_cat_fea_imp.csv'\n",
        "    cat_fea_imp.to_csv(file_name)\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_fea_result_df.csv'\n",
        "    fea_result_df.to_csv(file_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"EMBEDDED Complete Time =\", current_time)\n",
        "\n",
        "    # # #  EMBEDDED ============================== END =======================\n",
        "\n",
        "    #  MI ============================== Start =======================\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"MI Start Time =\", current_time)\n",
        "\n",
        "    mi_fea_imp, fea_result_df, mutual_info = gen_mutual_information(X_train, y_train, \\\n",
        "                                          xg, lg, cat, fea_result_df, \\\n",
        "                                          random_state = SEED)\n",
        "\n",
        "    print('MI fea result: ' + str(fea_result_df.shape[0]))\n",
        "\n",
        "    print('MI fea important: ' + str(mi_fea_imp.shape[0]))\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_mi_fea_imp.csv'\n",
        "    mi_fea_imp.to_csv(file_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"MI Complete Time =\", current_time)\n",
        "\n",
        "    # #  MI ============================== END =======================\n",
        "\n",
        "    # #  PI ============================== Start =======================\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"PI Start Time =\", current_time)\n",
        "      \n",
        "    pi_fea_imp, fea_result_df = gen_permutation_importance(X_train, y_train, \\\n",
        "                                                  xg, lg, cat, fea_result_df, \n",
        "                                                  SEED = SEED)\n",
        "\n",
        "    print('PI fea result: ' + str(fea_result_df.shape[0]))\n",
        "\n",
        "    print('PI fea important: ' + str(pi_fea_imp.shape[0]))\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_pi_fea_imp.csv'\n",
        "    pi_fea_imp.to_csv(file_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Complete Time =\", current_time)\n",
        "\n",
        "     # #  PI ============================== End =======================\n",
        "\n",
        "    # #  COM ============================== Start =======================\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"COM Start Time =\", current_time)\n",
        "\n",
        "    com_fea_imp, fea_result_df = gen_com_fea_importance(X_train, y_train, xg, lg, \\\n",
        "                                            cat, fea_result_df, mutual_info, SEED = SEED)\n",
        "\n",
        "    print('COM fea: ' + str(fea_result_df.shape[0]))\n",
        "\n",
        "    print('COM fea important: ' + str(com_fea_imp.shape[0]))\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_com_fea_imp.csv'\n",
        "    com_fea_imp.to_csv(file_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Complete Time =\", current_time)\n",
        "    # #  COM ============================== End =======================\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_fea_result_df.csv'\n",
        "    fea_result_df.to_csv(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dwm16InMoBe"
      },
      "outputs": [],
      "source": [
        "def create_fea_histo(fea_result_df, data_name):\n",
        "\n",
        "      sns.set(font_scale=1.5)\n",
        "\n",
        "      fea_result_df = fea_result_df[fea_result_df['Data Set'] == data_name]\n",
        "\n",
        "      plt.figure(figsize = (20,15))\n",
        "      lp = sns.lineplot(data=fea_result_df, x='num_feature', y='accuracy', hue='Model ',ci=None, marker='o')\n",
        "      lp.legend(loc='upper right',bbox_to_anchor=(1.19, 1))\n",
        "      plt.title('{}: Cross Validation Accuracy for All Feature Selection'.format(data_name))\n",
        "      plt.xlabel('Number of Feature')\n",
        "      plt.ylabel('Accuracy')\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzthA3jGLn9D"
      },
      "outputs": [],
      "source": [
        "def read_fea_result():\n",
        "\n",
        "      file_name = '/content/drive/MyDrive/Result/Feature_Selection_CV_All.csv'\n",
        "      return pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR-ro25fL-rQ"
      },
      "outputs": [],
      "source": [
        "if FEATURE_IMPORTANT == True:\n",
        "\n",
        "    for i in ['CDs', 'Books' ,'Electronics', 'Movies']:\n",
        "        fea_result_df = read_fea_result()\n",
        "\n",
        "        create_fea_histo(fea_result_df, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MDMH_nJ7Ma8"
      },
      "source": [
        "## Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99AThVSzSnAV"
      },
      "outputs": [],
      "source": [
        "def get_prediction_result(clf, X_train, X_test, y_train, y_test, pred_result, data_name):\n",
        "\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  y_pred = clf.predict(X_test)\n",
        "\n",
        "  print(clf.name + ' ' + data_name + str(X_train.shape))\n",
        "  print('Number of feature - ' + data_name + ' : ' + str(len(X_train.columns)) )\n",
        "\n",
        "  new_value = {'model': clf.name,\n",
        "               'data': data_name,\n",
        "              'accuracy': accuracy_score(y_test, y_pred),\n",
        "              'f1_score': f1_score(y_test, y_pred),\n",
        "              'precision': precision_score(y_pred = y_pred, y_true = y_test),\n",
        "              'recall': recall_score(y_pred = y_pred, y_true = y_test)}\n",
        "\n",
        "  pred_result = pred_result.append(new_value, ignore_index = True)\n",
        "\n",
        "  return pred_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7IxQIPAQDyW"
      },
      "outputs": [],
      "source": [
        "def run_prediction_mode (pred_result_df, pred_df, data_cato_name,fea_imp, num_fea, xg_params, lg_params, cat_params):\n",
        "\n",
        "  xg = SklearnHelper(clf=xgb.XGBClassifier, seed=SEED, \\\n",
        "                   params=xg_params, name = 'XGBoost')\n",
        "  lg = SklearnHelper(clf=lgb.LGBMClassifier, seed=SEED, \\\n",
        "                   params=lg_params, name = 'LightGBM')\n",
        "  cat = SklearnHelper(clf=CatBoostClassifier, seed=SEED, \\\n",
        "                    params=cat_params, name = 'CatBoost')\n",
        "\n",
        "  #All data\n",
        "  X_train_all, y_train_all, X_test_all, y_test_all, X_hpo_all, y_hpo_all = get_standard_data_split(pred_df.copy())\n",
        "\n",
        "  # 100k - almasdi\n",
        "  X_train, y_train, X_test, y_test, X_hpo, y_hpo = get_almasdi_data_split(pred_df.copy())\n",
        "\n",
        "  features = fea_imp.loc[0:num_fea-1,'Feature']\n",
        "  \n",
        "  # All data\n",
        "  X_train_all_fea = X_train_all[features].copy()\n",
        "\n",
        "  print('Number of Training feature {}'.format(len(X_train_all_fea.columns)))\n",
        "\n",
        "  X_test_all_fea = X_test_all[features].copy()\n",
        "\n",
        "  print('Number of Test feature {}'.format(len(X_test_all_fea.columns)))\n",
        "\n",
        "  #alma\n",
        "  X_train_fea = X_train[fea_imp.loc[0:num_fea-1,'Feature']].copy()\n",
        "\n",
        "  X_test_fea = X_test[fea_imp.loc[0:num_fea-1,'Feature']].copy()\n",
        "\n",
        "  for i in [xg, lg, cat]:\n",
        "    pred_result_df = get_prediction_result(i, X_train_all_fea, X_test_all_fea, y_train_all, \\\n",
        "                                          y_test_all, pred_result_df, data_cato_name + '_all')\n",
        "                                          \n",
        "    \n",
        "    pred_result_df = get_prediction_result(i, X_train_fea, X_test_fea, y_train, \\\n",
        "                                          y_test, pred_result_df, data_cato_name + '_100K')\n",
        "\n",
        "  return pred_result_df\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsPw0QfEi-AA"
      },
      "outputs": [],
      "source": [
        "def process_hy_result(model, data_cato_name, best_param, best_score, hy_result_df):\n",
        "\n",
        "  new_value = {\n",
        "      'model':model,\n",
        "        'data_cato_name': data_cato_name,\n",
        "        'best_parameter': best_param,\n",
        "        'best_score': best_score,\n",
        "        }\n",
        "\n",
        "  hy_result_df = hy_result_df.append(new_value,ignore_index = True)\n",
        "\n",
        "  return hy_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtdbpNwJf6r0"
      },
      "outputs": [],
      "source": [
        "def read_fea_imp(data_cato_name, tech):\n",
        "\n",
        "    if tech == 'pi':\n",
        "      file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_pi_fea_imp.csv'\n",
        "      return pd.read_csv(file_name)\n",
        "\n",
        "    elif tech == 'com':\n",
        "      file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_com_fea_imp.csv'\n",
        "      return pd.read_csv(file_name)\n",
        "\n",
        "    elif tech == 'mi':\n",
        "      file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_mi_fea_imp.csv'\n",
        "      return pd.read_csv(file_name)\n",
        "\n",
        "    elif tech == 'majority':\n",
        "      file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_majority_fea_imp.csv'\n",
        "      return pd.read_csv(file_name)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck-z3K1L051K"
      },
      "outputs": [],
      "source": [
        "def create_pred_result_histo(pred_result_df):\n",
        "\n",
        "      for i in ['Accuracy', 'Precision', 'Recall', 'F1 Score']:\n",
        "\n",
        "          if i == 'Accuracy':\n",
        "              y = 'accuracy'\n",
        "          elif i == 'Precision':\n",
        "              y = 'precision'\n",
        "          elif i == 'Recall':\n",
        "              y = 'recall'\n",
        "          elif i == 'F1 Score':\n",
        "              y = 'f1_score'\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(20, 7))\n",
        "\n",
        "          sns.set(font_scale=1.2)\n",
        "\n",
        "          p = sns.barplot(data=pred_result_df, \\\n",
        "                        x=\"data\", y=y, hue='model')\n",
        "          \n",
        "          show_values(p)\n",
        "\n",
        "          plt.title('{} by Data Set and Model - {}'.format(i, data_cato_name))\n",
        "          plt.xlabel('Data')\n",
        "          plt.ylabel(i)\n",
        "          plt.ylim(0.80, 1.00)\n",
        "          plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLOOysVM3P2E"
      },
      "outputs": [],
      "source": [
        "def process_cv_result(model, cv_result, data_train_test, data_all_100k, data_cato_name, cv_result_df):\n",
        "\n",
        "  for i in cv_result:\n",
        "      new_value = {\n",
        "        'model': model,\n",
        "        'accuracy': i,\n",
        "        'data_train_test': data_train_test,\n",
        "        'data_all_100k': data_all_100k,\n",
        "        'dataset': data_cato_name,\n",
        "        }\n",
        "\n",
        "      cv_result_df = cv_result_df.append(new_value,ignore_index = True)\n",
        "\n",
        "  return cv_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frRSxiCIk9rh"
      },
      "outputs": [],
      "source": [
        "def get_cv_result(i, data_cato_name,fea_imp, num_fea,xg_params, lg_params, cat_params, cv_result_df):\n",
        "\n",
        "    xg = SklearnHelper(clf=xgb.XGBClassifier, seed=SEED, \\\n",
        "                      params=xg_params, name = 'XGBoost')\n",
        "    lg = SklearnHelper(clf=lgb.LGBMClassifier, seed=SEED, \\\n",
        "                      params=lg_params, name = 'LightGBM')\n",
        "    cat = SklearnHelper(clf=CatBoostClassifier, seed=SEED, \\\n",
        "                        params=cat_params, name = 'CatBoost')\n",
        "\n",
        "    #All data\n",
        "    X_train_all, y_train_all, X_test_all, y_test_all, X_hpo_all, y_hpo_all = get_standard_data_split(i.copy())\n",
        "\n",
        "    # 100k - almasdi\n",
        "    X_train, y_train, X_test, y_test, X_hpo, y_hpo = get_almasdi_data_split(i.copy())\n",
        "\n",
        "    features = fea_imp.loc[0:num_fea-1,'Feature']\n",
        "\n",
        "    print('Number of features: {}'.format(len(features)))\n",
        "      \n",
        "    #only take 80 features for all data\n",
        "    X_train_all_fea = X_train_all[features].copy()\n",
        "\n",
        "    X_test_all_fea = X_test_all[features].copy()\n",
        "\n",
        "    #only take 80 features for alma\n",
        "    X_train_fea = X_train[features].copy()\n",
        "\n",
        "    X_test_fea = X_test[features].copy()\n",
        "\n",
        "    print('X_train_all_fe: {}, X_test_all_fea: {}, X_train_fea: {}, X_test_fea \\\n",
        "    {}'.format(len(X_train_all_fea.columns),len(X_test_all_fea.columns),\\\n",
        "               len(X_train_fea.columns),len(X_test_fea.columns)))\n",
        "\n",
        "# XG Boost - all data ===================================================\n",
        "    print('XGBoost Cross Validation Start All Data- ' + data_cato_name)\n",
        "    now = datetime.now()\n",
        "\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Current Time =\", current_time)\n",
        "\n",
        "    cv_result = xg.cross_validation(X_train_all_fea, y_train_all)\n",
        "\n",
        "    cv_result_df = process_cv_result('XGBoost', cv_result, 'train', 'all', data_cato_name,cv_result_df)\n",
        "\n",
        "    print('XGBoost Cross Validation End All Data- ' + data_cato_name)\n",
        "# XG Boost - all data - end =============================================\n",
        "\n",
        "# XG Boost - 100K data - Start========================================\n",
        "    print('XGBoost Cross Validation Start 100K Data- ' + data_cato_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Current Time =\", current_time)\n",
        "\n",
        "    cv_result = xg.cross_validation(X_train_fea, y_train)\n",
        "\n",
        "    cv_result_df = process_cv_result('XGBoost', cv_result, 'train', '100k', data_cato_name,cv_result_df)\n",
        "\n",
        "    print('XGBoost Cross Validation End 100K Data- ' + data_cato_name)\n",
        "# XG Boost - 100K data - End========================================\n",
        "\n",
        "# Ligth GBM- all data - Start========================================\n",
        "    print('LightGBM Cross Validation Start All Data- ' + data_cato_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Current Time =\", current_time)\n",
        "\n",
        "    cv_result =  lg.cross_validation(X_train_all_fea, y_train_all)\n",
        "\n",
        "    cv_result_df = process_cv_result('LightGBM', cv_result, 'train', 'all', data_cato_name,cv_result_df)\n",
        "\n",
        "    print('LightGBM Cross Validation End All Data- ' + data_cato_name)\n",
        "# Ligth GBM- all data - End ========================================\n",
        "\n",
        "\n",
        "# Ligth GBM- 100k data - Start========================================\n",
        "    print('LightGBM Cross Validation Start 100K Data- ' + data_cato_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Current Time =\", current_time)\n",
        "\n",
        "    cv_result =  lg.cross_validation(X_train_fea, y_train)\n",
        "\n",
        "    cv_result_df = process_cv_result('LightGBM', cv_result, 'train', '100K', data_cato_name,cv_result_df)\n",
        "\n",
        "    print('LightGBMCross Validation End 100K Data- ' + data_cato_name)\n",
        "# Ligth GBM- 100k data - End========================================\n",
        "\n",
        "# CatBoost- all data - Start========================================\n",
        "    print('CatBoost Cross Validation Start All Data- ' + data_cato_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Current Time =\", current_time)\n",
        "\n",
        "    cv_result =  cat.cross_validation(X_train_all_fea, y_train_all)\n",
        "\n",
        "    cv_result_df = process_cv_result('CatBoost', cv_result, 'train', 'all', data_cato_name,cv_result_df)\n",
        "\n",
        "    print('CatBoost Cross Validation End All Data- ' + data_cato_name)\n",
        "# CatBoost- all data - End ========================================\n",
        "\n",
        "# CatBoost- 100k data - Start========================================\n",
        "    print('CatBoost Cross Validation Start 100K Data- ' + data_cato_name)\n",
        "\n",
        "    now = datetime.now()\n",
        "\n",
        "    current_time = now.strftime(\"%H:%M:%S\")\n",
        "    print(\"Current Time =\", current_time)\n",
        "\n",
        "    cv_result =  cat.cross_validation(X_train_fea, y_train)\n",
        "\n",
        "    cv_result_df = process_cv_result('CatBoost', cv_result, 'train', '100k', data_cato_name,cv_result_df)\n",
        "\n",
        "    print('CatBoost Cross Validation End 100K Data- ' + data_cato_name)\n",
        "# CatBoost- 100k data - End========================================\n",
        "\n",
        "    return cv_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAPHwfCKJ01W"
      },
      "outputs": [],
      "source": [
        "def run_prediction_mode (X_train_all, y_train_all, X_test_all, y_test_all, X_train_100K, y_train_100K, \\\n",
        "                         X_test_100K, y_test_100K, name, pred_result_df, feature_selection, fea_imp_df, best_fea_hpo_df):\n",
        "\n",
        "  if feature_selection == 'mi' or feature_selection == 'pi' or feature_selection == 'com':\n",
        "\n",
        "    print('Start Prediction: {}'.format(feature_selection))\n",
        "\n",
        "    for model in ['XGBoost', 'LightGBM', 'CatBoost']: \n",
        "\n",
        "        print('Start Prediction: {}'.format(model))\n",
        "\n",
        "        best_setup = best_fea_hpo_df[(best_fea_hpo_df['data_cato_name'] == name) & (best_fea_hpo_df['model'] == '{}_{}'.format(model,feature_selection))]\n",
        "\n",
        "        num_features = best_setup['Best_num_feature'].iloc[0]\n",
        "        print('Best number of Feature: {}'.format(num_features))\n",
        "\n",
        "        features = fea_imp_df.loc[0:num_features-1,'Feature']\n",
        "\n",
        "        print('Number of Feature from feature important: {}'.format(len(features)))\n",
        "\n",
        "        # All data\n",
        "        X_train_all_fea = X_train_all[features].copy()\n",
        "\n",
        "        print('Number of column 100K  Training feature {}'.format(X_train_all_fea.shape))\n",
        "\n",
        "        X_test_all_fea = X_test_all[features].copy()\n",
        "\n",
        "        print('Number of column 100K  Test feature {}'.format(X_train_all_fea.shape))\n",
        "\n",
        "        #alma\n",
        "        X_train_fea = X_train_100K[features].copy()\n",
        "\n",
        "        X_test_fea = X_test_100K[features].copy()\n",
        "\n",
        "        params = best_setup['best_parameter'].iloc[0]\n",
        "\n",
        "        params = ast.literal_eval(params)\n",
        "\n",
        "        print('Best Params: {}'.format(params))\n",
        "\n",
        "        if model == 'XGBoost':\n",
        "            clf_model = SklearnHelper(clf=xgb.XGBClassifier, seed=SEED, \\\n",
        "                   params=params, name = 'XGBoost')\n",
        "\n",
        "        elif model == 'LightGBM':\n",
        "            clf_model = SklearnHelper(clf=lgb.LGBMClassifier, seed=SEED, \\\n",
        "                   params=params, name = 'LightGBM')\n",
        "\n",
        "        elif model == 'CatBoost':\n",
        "            clf_model = SklearnHelper(clf=CatBoostClassifier, seed=SEED, \\\n",
        "                    params=params, name = 'CatBoost')\n",
        "\n",
        "        pred_result_df = get_prediction_result(clf_model, X_train_all_fea, X_test_all_fea, y_train_all, y_test_all, pred_result_df, '{}_{}_{}_all'.format(name, model,feature_selection))\n",
        "                                                \n",
        "          \n",
        "        pred_result_df = get_prediction_result(clf_model, X_train_fea, X_test_fea, y_train_100K, y_test_100K, pred_result_df, '{}_{}_{}_100K'.format(name, model,feature_selection))\n",
        "\n",
        "        print(pred_result_df)\n",
        "\n",
        "  elif feature_selection == 'rfe':\n",
        "\n",
        "    print('Start Prediction: {}', feature_selection)\n",
        "\n",
        "    for model in ['XGBoost', 'LightGBM', 'CatBoost']:\n",
        "\n",
        "        print('Start Prediction: {}', model)\n",
        "\n",
        "        best_setup = best_fea_hpo_df[(best_fea_hpo_df['data_cato_name'] == name) & (best_fea_hpo_df['model'] == '{}_{}'.format(model,feature_selection))]\n",
        "\n",
        "        fea_rfe = best_setup['best_feature'].iloc[0]\n",
        "                            \n",
        "        for character in '[]':\n",
        "\n",
        "            fea_rfe = fea_rfe.replace(character, '')\n",
        "\n",
        "        fea_rfe = fea_rfe.split()\n",
        "\n",
        "        fea_rfe = np.array([ast.literal_eval(x.title()) for x in fea_rfe])\n",
        "\n",
        "        features = X_train_all.columns[fea_rfe]\n",
        "\n",
        "        print(name)\n",
        "\n",
        "        print(features)\n",
        "\n",
        "        print('Number of features: {}'.format(features))\n",
        "\n",
        "        # All data\n",
        "        X_train_all_fea = X_train_all[features].copy()\n",
        "\n",
        "        print('Number of shape Training feature {}'.format(X_train_all_fea.shape))\n",
        "\n",
        "        X_test_all_fea = X_test_all[features].copy()\n",
        "\n",
        "        print('Number of columns Test feature {}'.format(X_train_all_fea.shape))\n",
        "\n",
        "        #alma\n",
        "        X_train_fea = X_train_100K[features].copy()\n",
        "\n",
        "        print('Number of column 100K Training feature {}'.format(X_train_fea.shape))\n",
        "\n",
        "        X_test_fea = X_test_100K[features].copy()\n",
        "\n",
        "        print('Number of column 100K  Training feature {}'.format(X_test_fea.shape))\n",
        "\n",
        "        params = best_setup['best_parameter'].iloc[0]\n",
        "\n",
        "        params = ast.literal_eval(params)\n",
        "\n",
        "        print('Best Params: {}'.format(params))\n",
        "\n",
        "        if model == 'XGBoost':\n",
        "            clf_model = SklearnHelper(clf=xgb.XGBClassifier, seed=SEED, \\\n",
        "                  params=params, name = 'XGBoost')\n",
        "\n",
        "        elif model == 'LightGBM':\n",
        "            clf_model = SklearnHelper(clf=lgb.LGBMClassifier, seed=SEED, \\\n",
        "                  params=params, name = 'LightGBM')\n",
        "\n",
        "        elif model == 'CatBoost':\n",
        "            clf_model = SklearnHelper(clf=CatBoostClassifier, seed=SEED, \\\n",
        "                    params=params, name = 'CatBoost')\n",
        "\n",
        "        pred_result_df = get_prediction_result(clf_model, X_train_all_fea, X_test_all_fea, y_train_all, y_test_all, pred_result_df, '{}_{}_{}_all'.format(name, model,feature_selection))\n",
        "\n",
        "        print(pred_result_df)\n",
        "                                                \n",
        "        pred_result_df = get_prediction_result(clf_model, X_train_fea, X_test_fea, y_train_100K, y_test_100K, pred_result_df, '{}_{}_{}_100K'.format(name, model,feature_selection))\n",
        "\n",
        "        print(pred_result_df)\n",
        "\n",
        "  return pred_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWQo91F4_nJL"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "def hyperparameter_tuning(X, y, name, hy_result_df, feature_selection, best_fea_result_df, fea_imp_df):\n",
        "\n",
        "  if feature_selection == 'mi' or feature_selection == 'pi' or feature_selection == 'com':\n",
        "\n",
        "    best_num_fea = best_fea_result_df[best_fea_result_df['technique'] == feature_selection]\n",
        "\n",
        "    print(best_num_fea)\n",
        "\n",
        "    xg_features = best_num_fea[best_num_fea['model'] == 'XGBoost' ]['num_feature'].iloc[0]\n",
        "    lgm_features = best_num_fea[best_num_fea['model'] == 'LightGBM' ]['num_feature'].iloc[0]\n",
        "    cat_features = best_num_fea[best_num_fea['model'] == 'CatBoost' ]['num_feature'].iloc[0]\n",
        "\n",
        "  elif feature_selection == 'rfe':\n",
        "\n",
        "    best_xgb_num_fea = best_fea_result_df[(best_fea_result_df['technique'] == feature_selection) & (best_fea_result_df['model'] == 'XGBoost')]['num_feature'].iloc[0]\n",
        "    best_lgm_num_fea = best_fea_result_df[(best_fea_result_df['technique'] == feature_selection) & (best_fea_result_df['model'] == 'LightGBM')]['num_feature'].iloc[0]\n",
        "    best_cat_num_fea = best_fea_result_df[(best_fea_result_df['technique'] == feature_selection) & (best_fea_result_df['model'] == 'CatBoost')]['num_feature'].iloc[0]\n",
        "\n",
        "    print(best_xgb_num_fea, best_lgm_num_fea, best_cat_num_fea)\n",
        "\n",
        "    xgb_rfe = fea_imp_df[(fea_imp_df['no_feature'] == best_xgb_num_fea) & (fea_imp_df['model'] == 'XGBoost')]['support_of_each_feature'].iloc[0]\n",
        "    lgm_rfe = fea_imp_df[(fea_imp_df['no_feature'] == best_lgm_num_fea) & (fea_imp_df['model'] == 'LightGBM')]['support_of_each_feature'].iloc[0]\n",
        "    cat_rfe = fea_imp_df[(fea_imp_df['no_feature'] == best_cat_num_fea) & (fea_imp_df['model'] == 'CatBoost')]['support_of_each_feature'].iloc[0]\n",
        "\n",
        "    for character in '[]':\n",
        "        xgb_rfe = xgb_rfe.replace(character, '')\n",
        "        lgm_rfe = lgm_rfe.replace(character, '')\n",
        "        cat_rfe = cat_rfe.replace(character, '')\n",
        "\n",
        "    xgb_rfe = xgb_rfe.split()\n",
        "    lgm_rfe = lgm_rfe.split()\n",
        "    cat_rfe = cat_rfe.split()\n",
        "\n",
        "    xgb_rfe = np.array([ast.literal_eval(x.title()) for x in xgb_rfe])\n",
        "    lgm_rfe = np.array([ast.literal_eval(x.title()) for x in lgm_rfe])\n",
        "    cat_rfe = np.array([ast.literal_eval(x.title()) for x in cat_rfe])\n",
        "      \n",
        "# ========== XGBoost Hyperparameter ==== Start=================\n",
        "\n",
        "  now = datetime.now()\n",
        "\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "  print('Start XGBoost {}: {}'.format(feature_selection, name))\n",
        "  print(\"Current Time =\", current_time)\n",
        "\n",
        "  xg_hyper_grid = {\n",
        "               'max_depth': [2,3,5,7,10],\n",
        "               'gamma': [0, 0.1, 0.2, 0.3, 0.4, 1.0, 1.5, 2.0],\n",
        "               'learning_rate': [0.025,0.05,0.1,0.2,0.3],\n",
        "               'subsample': [0.15, 0.5, 0.75, 1.0],\n",
        "               'colsample_bylevel': [0.1, 0.2, 0.25, 0.5, 0.75, 1.0],\n",
        "              'tree_method': ['gpu_hist'],\n",
        "              }\n",
        "\n",
        "  #Set up the random search with 5-fold cross validation\n",
        "\n",
        "  xgb_clf = xgb.XGBClassifier(random_state = SEED, tree_method = 'gpu_hist', verbose=500)\n",
        "\n",
        "  xgb_rand_cv = RandomizedSearchCV(estimator=xgb_clf,\n",
        "             param_distributions=xg_hyper_grid,\n",
        "             cv=10, n_iter=60,\n",
        "             verbose = -1, random_state = SEED)\n",
        "  \n",
        "  if feature_selection == 'rfe':\n",
        "      feature = X.columns[xgb_rfe]\n",
        "\n",
        "      print('Number of Feature:' + str(len(feature)))\n",
        "\n",
        "      X_fea = X[feature].copy()\n",
        "  else:\n",
        "      X_fea = X[fea_imp_df.loc[0:xg_features-1,'Feature']].copy()\n",
        "  \n",
        "  print('Number of feature - ' + name + ' : ' + str(len(X_fea.columns)))\n",
        "  print('Shape: {}'.format(str(X_fea.shape)))\n",
        "\n",
        "  xgb_rand_cv.fit(X_fea, y)\n",
        "\n",
        "  print('XGBoost {} Best score reached for {}: {} with params: {} '.format(feature_selection, name, \\\n",
        "                                        xgb_rand_cv.best_score_, xgb_rand_cv.best_params_))\n",
        "  \n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "  print('End XGBoost {}: {}'.format(feature_selection, name))\n",
        "  print(\"Current Time =\", current_time)\n",
        "\n",
        "  hy_result_df = process_hy_result('XGBoost_{}'.format(feature_selection), name, xgb_rand_cv.best_params_, xgb_rand_cv.best_score_, hy_result_df)\n",
        "\n",
        "  print(hy_result_df)\n",
        "\n",
        "# # ========== XGBoost Hyperparameter ==== End =================\n",
        "\n",
        "# # ========== LightGB< Hyperparameter ==== Start=================\n",
        "\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "  print('Start LightGBM {}: {}'.format(feature_selection, name))\n",
        "  print(\"Current Time =\", current_time)\n",
        "\n",
        "  lg_params = {'num_leaves': [3, 7, 15, 31], \n",
        "                'learning_rate': [0.025, 0.05, 0.1, 0.2, 0.3],\n",
        "                'top_rate': [0.2, 0.4, 0.6, 0.7],\n",
        "                'other_rate': [0.05, 0.1, 0.3],\n",
        "                'feature_fraction_bynode': [0.1, 0.2, 0.25, 0.5, 0.75, 1.0],\n",
        "               'device': 'gpu'\n",
        "              }\n",
        "\n",
        "\n",
        "  lgb_clf = lgb.LGBMClassifier(random_state = SEED, device = 'gpu',\n",
        "                               metric='None', \n",
        "                               verbose=-1)\n",
        "\n",
        "  # Initialize a RandomizedSearchCV object using 5-fold CV-\n",
        "  lgb_cv = RandomizedSearchCV(estimator=lgb_clf, \n",
        "                             param_distributions=lg_params, cv = 10, \n",
        "                             scoring='accuracy', \n",
        "                             n_iter=60, \n",
        "                             verbose=-1, random_state = SEED) \n",
        "  \n",
        "  if feature_selection == 'rfe':\n",
        "      feature = X.columns[lgm_rfe]\n",
        "\n",
        "      print('Number of Feature:' + str(len(feature)))\n",
        "\n",
        "      X_fea = X[feature].copy()\n",
        "  else:\n",
        "      X_fea = X[fea_imp_df.loc[0:lgm_features-1,'Feature']].copy()\n",
        "  \n",
        "  print('Number of feature - ' + name + ' : ' + str(len(X_fea.columns)))\n",
        "  print('Shape: {}'.format(str(X_fea.shape)))\n",
        "\n",
        "  # Train on training data-\n",
        "  lgb_cv.fit(X_fea, y)\n",
        "\n",
        "  print('LightGBM {} Best score reached for {}: {} with params: {} '.format(feature_selection, name, \\\n",
        "                                        lgb_cv.best_score_, lgb_cv.best_params_))\n",
        "  \n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "  print('End LightGBM {}: {}'.format(feature_selection, name))\n",
        "  print(\"Current Time =\", current_time)\n",
        "  \n",
        "  hy_result_df = process_hy_result('LightGBM_{}'.format(feature_selection), name, lgb_cv.best_params_, lgb_cv.best_score_, hy_result_df)\n",
        "\n",
        "  print(hy_result_df)\n",
        "\n",
        "## ========== LightGB< Hyperparameter ==== End =================\n",
        "\n",
        "# ========== CatBoost Hyperparameter ==== Start =================\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "  print('Start CatBoost {}: {}'.format(feature_selection, name))\n",
        "  print(\"Current Time =\", current_time)\n",
        "\n",
        "  cat_model = CatBoostClassifier(random_state=SEED, task_type='GPU', \n",
        "                                 eval_metric = 'Accuracy',\n",
        "                                 silent = True)\n",
        "\n",
        "  params = {\n",
        "      'leaf_estimation_iterations': [1, 10],\n",
        "      \"max_depth\":[3, 6, 9],\n",
        "      'learning_rate': [0.025, 0.05, 0.1, 0.2, 0.3],\n",
        "      'l2_leaf_reg': [1, 3, 6, 9],\n",
        "      'task_type': ['GPU'],\n",
        "  }\n",
        "\n",
        "  if feature_selection == 'rfe':\n",
        "      feature = X.columns[cat_rfe]\n",
        "\n",
        "      print('Number of Feature:' + str(len(feature)))\n",
        "\n",
        "      X_fea = X[feature].copy()\n",
        "  else:\n",
        "      X_fea = X[fea_imp_df.loc[0:cat_features-1,'Feature']].copy()\n",
        "\n",
        "  print('Number of feature - ' + name + ' : ' + str(len(X_fea.columns)))\n",
        "  print('Shape: {}'.format(str(X_fea.shape)))\n",
        "\n",
        "  randomized_search_result = cat_model.randomized_search(param_distributions = params,\n",
        "                  X= X_fea,\n",
        "                  y= y,\n",
        "                  cv=10,\n",
        "                  n_iter=60,\n",
        "                  verbose=False,\n",
        "                  partition_random_seed = SEED)\n",
        "  \n",
        "  print(randomized_search_result['params'])\n",
        "  print(randomized_search_result['cv_results'])\n",
        "\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "\n",
        "  print('End CatBoost {}: {}'.format(feature_selection, name))\n",
        "  print(\"Current Time =\", current_time)\n",
        "\n",
        "  hy_result_df = process_hy_result('CatBoost_{}'.format(feature_selection), name, randomized_search_result['params'], randomized_search_result, hy_result_df)\n",
        "\n",
        "  print(hy_result_df)\n",
        "\n",
        "# ========== CatBoost Hyperparameter ==== End =================\n",
        "\n",
        "  return hy_result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDXQgLgO7Re6"
      },
      "outputs": [],
      "source": [
        "if NORMAL_PREDICTION == True or HYPERPARAMETER == True or CROSSVALIDATION == True \\\n",
        "or FEATURE_MAJORITY_PREDICTION == True or CV_FEATURE_MAJORITY_PREDICTION == True: \n",
        "\n",
        "  j = 0\n",
        "  for dataset in [cd_df,elec_df, movie_df, book_df]:\n",
        "\n",
        "    pred_result_df = pd.DataFrame({\n",
        "      'model':[],\n",
        "      'data':[],\n",
        "      'accuracy': [],\n",
        "      'f1_score': [],\n",
        "        'precision': [],\n",
        "        'recall': []\n",
        "    })\n",
        "\n",
        "    j += 1\n",
        "\n",
        "    if j == 1:\n",
        "      data_cato_name = 'CDs'\n",
        "      \n",
        "    elif j == 4:\n",
        "      data_cato_name = 'Books'\n",
        "      \n",
        "    elif j == 2:\n",
        "      data_cato_name = 'Electronics'\n",
        "      \n",
        "    elif j == 3:\n",
        "      data_cato_name = 'Movie'\n",
        "\n",
        "    if HYPERPARAMETER == True:\n",
        "\n",
        "      hy_result_df = pd.DataFrame({\n",
        "      'model':[],\n",
        "        'data_cato_name': [],\n",
        "        'best_parameter': [],\n",
        "        })\n",
        "      \n",
        "      X_train_all, y_train_all, X_test_all, y_test_all, X_hpo_all, y_hpo_all = get_standard_data_split(dataset.copy())\n",
        "\n",
        "      for feature_selection in ['mi','pi','com','rfe']:\n",
        "          file_name = '/content/drive/MyDrive/Result/best_fea_result.csv'\n",
        "\n",
        "          best_fea_result_df = pd.read_csv(file_name)\n",
        "\n",
        "          best_fea_result_df = best_fea_result_df[best_fea_result_df['Data Set'] == data_cato_name]\n",
        "\n",
        "          file_name = '/content/drive/MyDrive/Result/{}_{}_fea_imp.csv'.format(data_cato_name,feature_selection)\n",
        "\n",
        "          fea_imp_df = pd.read_csv(file_name)\n",
        "\n",
        "          hy_result_df = hyperparameter_tuning(X_hpo_all, y_hpo_all, data_cato_name, hy_result_df, feature_selection = feature_selection, best_fea_result_df = best_fea_result_df, fea_imp_df = fea_imp_df)\n",
        "\n",
        "      file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_hy.csv'\n",
        "      hy_result_df.to_csv(file_name)\n",
        "\n",
        "    if NORMAL_PREDICTION == True:\n",
        "\n",
        "        # All Data\n",
        "        X_train_all, y_train_all, X_test_all, y_test_all, X_hpo_all, y_hpo_all = get_standard_data_split(dataset.copy())\n",
        "\n",
        "        # 100k - almasdi\n",
        "        X_train_100K, y_train_100K, X_test_100K, y_test_100K, X_hpo_100K, y_hpo_100K = get_almasdi_data_split(dataset.copy())\n",
        "\n",
        "        file_name = '/content/drive/MyDrive/Result/combine_best_num_best_feature_hpo.csv'\n",
        "\n",
        "        best_fea_hpo_df = pd.read_csv(file_name)\n",
        "\n",
        "        for feature_selection in ['mi','pi','com','rfe']:\n",
        "            file_name = '/content/drive/MyDrive/Result/{}_{}_fea_imp.csv'.format(data_cato_name,feature_selection)\n",
        "\n",
        "            fea_imp_df = pd.read_csv(file_name)\n",
        "\n",
        "            pred_result_df = run_prediction_mode (X_train_all, y_train_all, X_test_all, y_test_all, X_train_100K, y_train_100K, \\\n",
        "                          X_test_100K, y_test_100K, data_cato_name, pred_result_df, feature_selection, fea_imp_df, best_fea_hpo_df)\n",
        "            \n",
        "            file_name = '/content/drive/MyDrive/Result/' + data_cato_name + '_pred_result.csv'\n",
        "            pred_result_df.to_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7pXDR3-NW_N"
      },
      "outputs": [],
      "source": [
        "def show_combine_pred_result_histogram():\n",
        "\n",
        "  file_name = '/content/drive/MyDrive/Result/combine_pred_result.csv'\n",
        "  pred_result = pd.read_csv(file_name)\n",
        "\n",
        "  for j in ['CDs', 'Books','Electronics', 'Movies']:\n",
        "\n",
        "      show_df = pred_result[pred_result['Category'] == j]\n",
        "\n",
        "      w = 1\n",
        "\n",
        "      for i in ['100K', 'All']:\n",
        "\n",
        "          data_df = show_df[show_df['Data Set'] == i].copy()\n",
        "\n",
        "          fig, ax = plt.subplots(figsize=(35, 13))\n",
        "\n",
        "          sns.set(font_scale=1.75)\n",
        "\n",
        "          p = sns.barplot(x = 'data', y = 'accuracy', data = data_df, hue='model')\n",
        "          \n",
        "          show_values(p)\n",
        "\n",
        "          if w == 1:\n",
        "             plt.title('{}: Accuracy by Model and Feature Selection Techniques of 100K Data Set'.format(j))\n",
        "          else:\n",
        "             plt.title('{}: Accuracy by Model and Feature Selection Techniques of All Data Set'.format(j))\n",
        "\n",
        "          plt.xlabel('Data')\n",
        "          plt.ylabel(i)\n",
        "          plt.ylim(0.75, 1.00)\n",
        "          plt.show()\n",
        "\n",
        "          w += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L03OLD_iPA3l"
      },
      "outputs": [],
      "source": [
        "if SHOW_PRED_RESULT == True:\n",
        "  show_combine_pred_result_histogram()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "FeatureImportance_HyperParameter_Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}